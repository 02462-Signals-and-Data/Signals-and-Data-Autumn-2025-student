{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# N-grams, Fastttext, and GloVE\n",
    "\n",
    "*This assignment focuses on exploring Fasttext and GloVE as NLP methods. We are going to focus on two tasks and ways of understanding models:*\n",
    "\n",
    "1. *The traditional, \"model is a classifier\" viewpoint. Here we are going to work with the [AG News Dataset](https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset) to classify genres*\n",
    "2. *The more vector-based way, seeing them basically as machines that just generate word vectors, with everything else just being gravy. Barring attaching a specific classifier, GloVE falls entirely under this category.* \n",
    "\n",
    "\n",
    "Task will be probably be about\n",
    "\n",
    "- Creating N-gram function (character-wise and word-wise)\n",
    "- Performing analysis on the output of the pre-made fassttext model\n",
    "- Training own linear classifier layer of the fasttext model (perhaps too difficult?) - Just create torch.linear, extract weights, etc.\n",
    "- Perhaps a lot of description about HOW fasttext and Word2Vec skipgram models work?\n",
    "- Performing PCA and feeding this to Michaels Fasttext model?\n",
    "- Perhaps crate simple model like naive bayes to classify texts based on PCA and cossim - like final project in 2021, only this time, a lot of the work can be done beforehand\n",
    "\n",
    "\n",
    "\n",
    "**TODO: Theoretical Questions**\n",
    "\n",
    "- Explain a CBOW and Skipgram model\n",
    "- Fasttext can technically train in only a semi-supervised manner, that means without labeled text, why is this, and why is this useful?\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "from scipy import signal\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import string\n",
    "import random\n",
    "from sklearn.metrics import classification_report,accuracy_score,balanced_accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "import seaborn as snb\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import torch\n",
    "import re\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "\n",
    "# TODO: MOVE THIS\n",
    "import scipy.spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Seed generator function\n",
    "# Generates robust seed values using methods adapted from Gaius-quantum reverse...\n",
    "# ...GaunTLets, see more https://isotropic.org/papers/chicken.pdf and explained https://www.youtube.com/watch?v=dQw4w9WgXcQ\n",
    "# Values are generated from a specific subset of alphanumerics representing sub-deca natural-numericals\n",
    "# from the glove.42B.300d.txt Use this subset for the reverse function as well, the whole one will take too long\n",
    "\n",
    "def generate_seed():\n",
    "    with open(\"important_stuff.pkl\", \"rb\") as fp:\n",
    "        GQRGaunTLets_69B_300_seed_vals = pickle.load(fp)\n",
    "        seed = int(np.mean(GQRGaunTLets_69B_300_seed_vals[69]))\n",
    "        return seed\n",
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    try: torch.manual_seed(seed_value)\n",
    "    except: pass\n",
    "\n",
    "seed_everything(generate_seed())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Word- and character-wise n-grams\n",
    "\n",
    "*As you know, n-grams are pretty useful for improving the otherwise limited bag-of-words (BoW) model. Most often, this is by making distinctions between sentences such as \"good\" and \"not good\" which would be represented somewhat the same in a regular BoW. It is very obvious if we consider the sentence \"Maria stole the milk\" vs \"The milk stole Maria\", two sentences completely identical in the BoW representation, but with two obviously different meanings.*\n",
    "\n",
    "*As you also know, Fasttext takes this further by creating chracter-wise n-grams. These are made up of n-characters of a single word. This allows fasttext to consider cases such as grammar, where words are spelled similarly and even consider misspellings, if someone makes a mistaek in wirtign a wrod, the character-wise n-gram representation will be **almost** the same as the correct word.*\n",
    "\n",
    "This is done by Fasttext simply storing embedding vectors $v_n$ for each n-gram, character or otherwise. Fasttext will simply then average all of these vectors to create the representation for a given text or sentence.\n",
    "\n",
    "$$v_{total} = \\frac{1}{N}\\sum^N_{n=0} v_n$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Wordwise n-grams and characterwise n-grams\n",
    "\n",
    "Using N-grams is a way of gathering information about specific combinations of words. For example 'not good' is very different from 'good'. It is also a primitive way of captuing the ordering of the words. Say we have a four-word sentence saying 'Maria stole the Milk', this would be the same as 'The milk stole Maria', even though the sentences are quite different... N-grams fix this by making the ordering of the words important.\n",
    "\n",
    "As you also know: Fasttext takes this one step further: By creating character-wise n-grams. These are made up of n-characters of a single word. This allows fasttext to not only look at combinations of specific words, but at combinations of these specific character n-grams. This is very useful for capturing information about prefixes or suffixes, such as 'dissimilar' vs 'similar' or 'pickle' vs 'pickles' - words that are very much alike, but would be considered as different by other methods.\n",
    "\n",
    "Specifically, fasttext creates a full embedding vector using character grams as a weighted average of every character gram in the word.\n",
    "\n",
    "$$v_{total} = \\frac{1}{N}\\sum^N_{n=0} v_n$$\n",
    "\n",
    "Every part of a word that is not in the current vocabulary of a the fasttext model, will simply have its $v_n$ set as a zero vector\n",
    "\n",
    "This also helps in spell-checking, so even if someone maeks a mistaek her and thare, fattext wil still be abel to understnd the text because the character n-grams are almost the same.\n",
    "\n",
    "**Question: Why do we use n-grams for text classification, and what particular strengths are there in using character-grams?**\n",
    "\n",
    "$\\dots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# N-gram functions -  Might need to be filled by students?\n",
    "\n",
    "def get_n_grams(text, n, lower=True, strip=True):\n",
    "    \"\"\"Gets a specific n-gram for a given text string\"\"\"\n",
    "    if lower:\n",
    "        text = text.lower()\n",
    "    if strip:\n",
    "        text = re.sub('[^A-Za-z0-9 ]+', '', text)\n",
    "\n",
    "    text = text.split()\n",
    "    n_grams = []\n",
    "\n",
    "    for i, word in enumerate(text):\n",
    "        if i+n > len(text):\n",
    "            break\n",
    "        n_grams.append(text[i: i+n])\n",
    "\n",
    "    return n_grams\n",
    "\n",
    "def get_word_grams(word, n):\n",
    "    \"\"\"Gets the character wise n-grams for a single word\"\"\"\n",
    "    word_grams = []\n",
    "\n",
    "    # So really this is not something you should do for the actual model\n",
    "    # String concatenation in python is O(N+M) complexity, which is blazingly slow\n",
    "    # Probably nltk.ngrams function does it faster\n",
    "    # Fasttext always adds beginning of word and end of word tokens to the words it is n-gramming:\n",
    "    word = '<' + word + '>'\n",
    "\n",
    "    for i, character in enumerate(word):\n",
    "        if i+n > len(word):\n",
    "            break\n",
    "        word_grams.append(word[i:i+n])\n",
    "\n",
    "    return word_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-grams here: \n",
      "  [['he', 'turned', 'himself'], ['turned', 'himself', 'into'], ['himself', 'into', 'a'], ['into', 'a', 'pickle'], ['a', 'pickle', 'funniest'], ['pickle', 'funniest', 'shit'], ['funniest', 'shit', 'ive'], ['shit', 'ive', 'ever'], ['ive', 'ever', 'seen']]\n",
      "Word-grams here: \n",
      "  [['<he', 'he>'], ['<tu', 'tur', 'urn', 'rne', 'ned', 'ed>'], ['<hi', 'him', 'ims', 'mse', 'sel', 'elf', 'lf>'], ['<in', 'int', 'nto', 'to>'], ['<a>'], ['<pi', 'pic', 'ick', 'ckl', 'kle', 'le>'], ['<fu', 'fun', 'unn', 'nni', 'nie', 'ies', 'est', 'st>'], ['<sh', 'shi', 'hit', 'it>'], ['<iv', 'ive', 've>']]\n"
     ]
    }
   ],
   "source": [
    "# Now let us just test these functions on some toy text...\n",
    "text = \"He turned himself into a pickle... Funniest shit, ive ever seen!!!\"\n",
    "\n",
    "n_grams = get_n_grams(text, 3, lower=True, strip=True)\n",
    "word_grams = [get_word_grams(words[0], 3) for words in n_grams]\n",
    "\n",
    "print(\"N-grams here: \\n \", n_grams)\n",
    "\n",
    "print(\"Word-grams here: \\n \", word_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As you can see, even in this very small sentence, there are a ton of n-grams, and even more word-grams which is why practically, the Fasttext model often operates on what is known as a **'bucket size'** which defines the maximum number of possible word-grams avaliable in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Training and using the fasttext model\n",
    "\n",
    "\n",
    "<p style=\"text-align:center;\">\"(Almost) Never do yourself what some other chump has done better\" </p>\n",
    "<p style=\"text-align:center;\"> - Creed of the KID </p>\n",
    "\n",
    "*Obviously someone else has made a pretty well working [Fasttext module](https://fasttext.cc/). In this case, it is the team at Meta (Facebook, back then). Aside from how well it trains, is does have a few weird things about it, most notably that it requires .txt files to train (bvadr).*\n",
    "\n",
    "*For this exercise, we are going to focus on just tweaking minn and maxnn whihc control the minimum and maximum length for the character-grams.*\n",
    "\n",
    "*A complete list of model hyperparameters can be found in the file hypereparams.txt, along with (most) methods callable on the Fasttext model. Refer to this if you need inspiration on making your model interesting.*\n",
    "\n",
    "*Important note: If the model is asked for a word-vector not in its current vocabulary, it will give a zero-vector of the same dimension as the other vectors in its vocabulary; that way even extremely esoteric spelling errors do not 'break' the model due to vocabulary lookup errors, the words themselves will just not add anything to the prediction.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 120000 data points in the dataset, \n",
      "7600 different points in the test set, and the different labels are [0 1 2 3],\n",
      "these correspond to the categories: ['World' 'Sports' 'Business' 'Sci/Tec']\n",
      "\n",
      "Training class balances:\n",
      "World 0.25\n",
      "Sports 0.25\n",
      "Business 0.25\n",
      "Sci/Tec 0.25\n",
      "\n",
      "Test class balances:\n",
      "World 0.25\n",
      "Sports 0.25\n",
      "Business 0.25\n",
      "Sci/Tec 0.25\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load AG_news data\n",
    "\n",
    "news_data = np.load('./news_data.npz', allow_pickle=True)\n",
    "train_texts = news_data['train_texts']\n",
    "test_texts = news_data['test_texts']\n",
    "train_labels = news_data['train_labels']\n",
    "test_labels = news_data['test_labels']\n",
    "ag_news_labels = news_data['ag_news_label']\n",
    "\n",
    "print(f\"There are a total of {len(train_labels)} data points in the dataset, \\n\"\n",
    "        f\"{len(test_texts)} different points in the test set, and the different labels are {np.unique(train_labels)},\\n\"\n",
    "        f\"these correspond to the categories: {ag_news_labels}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Let's just ensure there are no unfair class balances in either training or testing...\n",
    "\n",
    "n_classes = len(ag_news_labels)\n",
    "print(\"Training class balances:\")\n",
    "for i,c in enumerate(ag_news_labels):\n",
    "    print(c,np.mean(train_labels==i))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Test class balances:\")\n",
    "for i,c in enumerate(ag_news_labels):\n",
    "    print(c,np.mean(test_labels==i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120000it [09:19, 214.31it/s]\n",
      "Read 4M words\n",
      "Number of words:  91297\n",
      "Number of labels: 4\n",
      "Progress: 100.0% words/sec/thread: 5467918 lr:  0.000000 avg.loss:  0.294857 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# Creating fasttext data set from current training data\n",
    "\n",
    "def txtify_data(train_texts, train_labels, ag_news_labels, save_path='training_data.txt'):\n",
    "    \"\"\"\n",
    "    Creates a .txt file compatible with a fasttext model\n",
    "\n",
    "    Args:\n",
    "        train_texts (_type_): _description_\n",
    "        train_labells (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    txt = \"\"\n",
    "    for i, (trains, tests) in tqdm(enumerate(zip(train_texts, train_labels))):\n",
    "        trains = trains.lower()\n",
    "        trains = re.sub('[^a-z0-9 ]+', '', trains)\n",
    "\n",
    "        txt = txt + f'__label__{ag_news_labels[tests]} {trains}\\n'\n",
    "\n",
    "    \n",
    "    f = open(save_path, mode='w')\n",
    "    f.write(txt)\n",
    "    f.close()\n",
    "\n",
    "    return save_path\n",
    "\n",
    "path_to_doc = txtify_data(train_texts, train_labels, ag_news_labels, save_path='training_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining fasttext hyperparameters\n",
    "char_gram_length_min = 3 # If set to zero, we only train word-grams\n",
    "char_gram_length_max = 6 # If set to zero, we only train word-grams\n",
    "num_word_grams = 1 # Default value\n",
    "verbose = True # Set to false if you don't want to see training statistics\n",
    "\n",
    "# Train fasttext_word_model and fasttext_char_model respectively\n",
    "fasttext_word_model = fasttext.train_supervised(path_to_doc, maxn=0, minn=0, verbose=verbose,\n",
    "                                                wordNgrams=num_word_grams)\n",
    "\n",
    "fasttext_char_model = fasttext.train_supervised(path_to_doc, maxn=char_gram_length_max, minn=char_gram_length_min,\n",
    "                                                verbose=verbose, wordNgrams=num_word_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how the subwords of the character model and the word model differ\n",
    "# get_subwords gets all character-gram 'parts' of the word specified...\n",
    "# ...as well as indices corresponding to the row of the given vector in the embedding matrix\n",
    "print(fasttext_word_model.get_subwords('cat'))\n",
    "print(fasttext_char_model.get_subwords('cat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sci/Tec\n"
     ]
    }
   ],
   "source": [
    "def test_prediction(test_text, test_label, model):\n",
    "    \"\"\"\n",
    "    Method for testing fasttext model\n",
    "    Model should be either the character model or the word model\n",
    "    \"\"\"\n",
    "    prediction = model.predict(test_text)\n",
    "    if prediction[0][0][9:] == test_label:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# Reason why we index the way we do: .predict outputs a tuple of certainty and the label, the label being __label__Business for example for business\n",
    "print(fasttext_word_model.predict('A cat in a hat')[0][0][9:])\n",
    "predicts = fasttext_word_model.predict(list(test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__label__Business': {'precision': 0.8796636889122438,\n",
       "  'recall': nan,\n",
       "  'f1score': 1.7593273778244876},\n",
       " '__label__Sports': {'precision': 0.9592152813629323,\n",
       "  'recall': nan,\n",
       "  'f1score': 1.9184305627258647},\n",
       " '__label__Sci/Tec': {'precision': 0.8887139107611548,\n",
       "  'recall': nan,\n",
       "  'f1score': 1.7774278215223096},\n",
       " '__label__World': {'precision': 0.9277628032345013,\n",
       "  'recall': nan,\n",
       "  'f1score': 1.8555256064690027}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.test_label(\"data_data_test_cleaned.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9076315789473685\n"
     ]
    }
   ],
   "source": [
    "# Make the testing loop here to obtain the accuracy when predicting labels:\n",
    "\n",
    "sum = 0\n",
    "for (test_text, test_label) in zip(test_texts, test_labels):\n",
    "    sum += test_model(test_text, ag_news_labels[test_label])\n",
    "\n",
    "accuracy = sum/len(test_texts)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 GloVe to create embeddings vectors\n",
    "\n",
    "[GloVe Paper here](https://aclanthology.org/D14-1162.pdf), [GloVe Project page here](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "GloVe is called a \"global log-bilinear regression model\" which combines the strengths of global matrix factorization and local context window methods.\n",
    "\n",
    "In English, this means it combines methods that work by collecting information on the entire corpus (like LSA), with other methods that capture more local patterns, essentially what we see with Fasttext that considers local n-grams. GloVe just considers \"context windows\" rather than an n-gram. Overall, what they want are nicely defined, linear relationships, decided by comparing the co-occurences of different words.\n",
    "\n",
    "The selling point really, is that while a run-of-the-mill neural network **may** be able to answer the questions: \"Skibidi is to Toilet as Fanum is to ...?\", it will not necessarily be able to do it in a linear manner. Therefore considering all the word vectors together in their latent space, may not yield good information. GloVe fixes this by keeping all vector substructures linear.\n",
    "\n",
    "Essentially, GloVe trains by mixing a [Skipgram model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) (just a neural network) with a function that works more on the entire corpus, while maintaining a weighting between the two. Because GloVe works best on huge corpora of data, we are not going to train it ourselves, but just use pretrained GloVe vectors, collected from their [project page](https://nlp.stanford.edu/projects/glove/). \n",
    "\n",
    "\n",
    "# GloVe does not use neural networks everywhere, in particular when using F, as it would \"obfuscate the linear strcutures they are trying to capture\", what linear structures are talked about and how would they be obfuscated?\n",
    "\n",
    "\n",
    "# Why can we not go the other way when doing embedding vectors? If you had to get a word from a given embedding vector, how would you go about it?\n",
    "\n",
    "# What prevents us from simply making a dictionary with vectors as keys and words as values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(glove_path):\n",
    "    \"\"\"\n",
    "    Loads a GloVE vectors from a given path\n",
    "    \"\"\"\n",
    "    glove = {}\n",
    "    \n",
    "    print(\"Creating GloVE dictionary...\")\n",
    "    with open(glove_path, 'r') as f:\n",
    "        for line in tqdm(f):\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], 'float32')\n",
    "            glove[word] = vector\n",
    "    \n",
    "    return glove\n",
    "\n",
    "def create_GloVE_vector(text, glove, dim=300):\n",
    "    \"\"\"\n",
    "    Creates a GloVE vector for a given text and GloVe\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub('[^a-z0-9 ]+', '', text)\n",
    "    text = text.split()\n",
    "\n",
    "    vector = np.zeros(dim)\n",
    "\n",
    "    for word in text:\n",
    "        if word in glove:\n",
    "            vector += glove[word]\n",
    "\n",
    "    # TODO: Check if we actually need to get the mean here\n",
    "    vector = np.mean(vector)\n",
    "    return vector\n",
    "\n",
    "def word_similarity(word1, word2, glove):\n",
    "    \"\"\"\n",
    "    Returns the cosine similarity between two words\n",
    "    \"\"\"\n",
    "\n",
    "    # Sanity check to ensure both words are in GloVE\n",
    "    if word1 not in glove or word2 not in glove:\n",
    "        raise ValueError(\"Both words must be in GloVe!\")\n",
    "\n",
    "    return 1 - scipy.spatial.distance.cosine(glove[word1], glove[word2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating GloVE dictionary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1042989it [00:24, 41799.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between cat and dog is 0.7885447835189361\n",
      "Similarity between cat and banana is 0.3027379785240919\n",
      "Similarity between cat and cat is 0.9999999891532333\n",
      "Similarity between camera and man is 0.36750208040174503\n",
      "Similarity between steel and beams is 0.35676179498755567\n",
      "Similarity between six and 6 is 0.6511695714552783\n"
     ]
    }
   ],
   "source": [
    "# Check word similarity between a few words\n",
    "glove = load_glove('glove.42B.300d.txt')\n",
    "\n",
    "word_pairs = [('cat', 'dog'), ('cat', 'banana'), ('cat', 'cat'), ('camera', 'man'), ('steel', 'beams'), ('six', '6')]\n",
    "\n",
    "for word1, word2 in word_pairs:\n",
    "    print(f\"Similarity between {word1} and {word2} is {word_similarity(word1, word2, glove)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Examining the emebdding vectors\n",
    "\n",
    "We musn't forget, that at its core, fasttext is a method functioning on word embedding vectors, which it obtains by a skipgram model using some clever tricks. As such, we can expect the embeddings that are created by the fasttext model to hold some information about the words they 'code for'. We now wish to perform PCA on the entire word-embedding matrix to see if the semantic difference in words is visible with only a few principal components\n",
    "\n",
    "If you want to read more about word2vec, I reccomend: [here first](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/): and: [here afterwards](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)\n",
    "\n",
    "\n",
    "\n",
    "**Explain shortly what you expect to find if we perform PCA on the matrix of word-embeddings, that is the matrix which holds a vector representation of each word in our vocabulary**\n",
    "\n",
    "*Your answer here*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The variance explained of the first 5 principal components is: 0.9495278000831604\n",
      "The dimensionality of each principal component is: (100,) and there are of course 100 of them\n"
     ]
    }
   ],
   "source": [
    "# Performing PCA on embedding matrix\n",
    "\n",
    "embedding_matrix = fasttext_model.get_input_matrix() # It's called the input matrix because it is essentially what is fed into the rest of the rest of the fasttext model\n",
    "\n",
    "# Why does sklearn's PCA run faster than np.linalg.eig? - Singular value decomposition\n",
    "pca = PCA()\n",
    "pca.fit(embedding_matrix)\n",
    "\n",
    "n = 5\n",
    "print(f'The variance explained of the first {n} principal components is: {np.sum(pca.explained_variance_ratio_[:n])}')\n",
    "print(f'The dimensionality of each principal component is: {pca.components_[0].shape} and there are of course {len(pca.components_)} of them')\n",
    "\n",
    "# #  This is one example where the code below is not feasible - the embedding matrix is massive!\n",
    "#cov = np.cov(embedding_matrix.T)\n",
    "#scipy.linalg.eig(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cos_sim(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Should give two vectors, obtain cosine simliarity for them\n",
    "    \"\"\"\n",
    "    cossim = np.dot(vec1, vec2)/(np.linalg.norm(vec1)*np.linalg.norm(vec2))\n",
    "    #cossim  = 1 - spatial.distance.cosine(vec1, vec2)\n",
    "\n",
    "    return cossim\n",
    "\n",
    "def get_vector_transform(word, n=2):\n",
    "    \"\"\"\n",
    "    Should given a specific word string, obtain fasttext's vector representation of that word and project it on the n first principal components\n",
    "    \"\"\"\n",
    "    word_vec = fasttext_model.get_word_vector(word)\n",
    "    if n == 0:\n",
    "        return word_vec\n",
    "\n",
    "    return pca.components_[:n]@word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Some plotting stuff here\n",
    "words = ['company', 'business', 'cat', 'software', 'microsoft']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Of course plotting vectors can only really be done in two or three dimensions max, for more dimensions, we can use the cosine similarity previously defined to measure how similar two word vectors are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company-software: 0.8904985189437866\n",
      "company-business: 0.9662047028541565\n",
      "company-world: -0.46810242533683777\n",
      "business-software: 0.7828420400619507\n",
      "business-business: 1.0000001192092896\n",
      "business-world: -0.6497917175292969\n",
      "cat-software: 0.5068366527557373\n",
      "cat-business: -0.12006817758083344\n",
      "cat-world: 0.7469577789306641\n",
      "software-software: 1.0\n",
      "software-business: 0.7828420400619507\n",
      "software-world: -0.11456459760665894\n",
      "microsoft-software: 0.9591502547264099\n",
      "microsoft-business: 0.8924571871757507\n",
      "microsoft-world: -0.2848648428916931\n"
     ]
    }
   ],
   "source": [
    "n = 2 # Number of principal components to do with\n",
    "to_compare = ['software', 'business', 'world'] # Three words, that should be labeled as three different things\n",
    "for word in words:\n",
    "    for comparison in to_compare:\n",
    "        print(f\"{word}-{comparison}: {cos_sim(get_vector_transform(word, n), get_vector_transform(comparison,n))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Another strength of fasttext: Spelling errors\n",
    "\n",
    "A good thing if we want to use fasttext on a character level, is that it will be able to understand spelling errors. We're going to test this now by replacing a bunch of letters in our test set randomly with other words and once more test the accuracy of the word-wise fasttext vs the character-wise fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "def dyslexibot(test_set, p=0.05, extra_scuffed=False):\n",
    "    \"\"\"\n",
    "    tHe AlMiGhTy dyslexibot(tm) replaces letters with probability p\n",
    "    extra_scuffed does what it says: it makes the replacements even harder to guess\n",
    "    \"\"\"\n",
    "\n",
    "    if extra_scuffed:\n",
    "        test_set_letters = np.array(list(set(''.join(test_texts)))) # Can replace with all letters currently in test set\n",
    "    else:\n",
    "        test_set_letters = np.array(list(string.ascii_lowercase)) # Can only replace with lowercase letters\n",
    "\n",
    "    new_test_set = [text.split(' ') for text in test_set.copy()]\n",
    "\n",
    "    for i, text in tqdm(enumerate(new_test_set)):\n",
    "        for r, word in enumerate(text):\n",
    "            word = list(word)\n",
    "            for t, letter in enumerate(word):\n",
    "                rand = random.uniform(0, 1)\n",
    "\n",
    "                if extra_scuffed and rand < p: # We replace even spaces!\n",
    "                    word[t] = np.random.choice(test_set_letters)\n",
    "                    #new_test_set[i][r] = np.random.choice(test_set_letters)\n",
    "\n",
    "                elif letter != ' ' and rand < p:\n",
    "                    word[t] = np.random.choice(test_set_letters)\n",
    "                    #new_test_set[i][r] = np.random.choice(test_set_letters)\n",
    "\n",
    "            text[r] = ''.join(word)\n",
    "        new_test_set[i] = ' '.join(text)\n",
    "    return np.array(new_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7600it [00:01, 6532.80it/s]\n"
     ]
    }
   ],
   "source": [
    "bad_test_texts = dyslexibot(test_texts, p=0.05, extra_scuffed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8543421052631579\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "for (test_text, test_label) in zip(bad_test_texts, test_labels):\n",
    "    sum += test_model(test_text, ag_news_labels[test_label])\n",
    "\n",
    "accuracy = sum/len(bad_test_texts)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "transform = (pca.components_[:2])\n",
    "\n",
    "comp = fasttext_model.get_word_vector('company')\n",
    "buis = fasttext_model.get_word_vector('business')\n",
    "stock = fasttext_model.get_word_vector('stock')\n",
    "soft = fasttext_model.get_word_vector('software')\n",
    "tech = fasttext_model.get_word_vector('technology')\n",
    "\n",
    "comp_trans = transform@comp\n",
    "buis_trans = transform@buis\n",
    "stock_trans = transform@stock\n",
    "soft_trans = transform@soft\n",
    "tech_trans = transform@tech\n",
    "\n",
    "\n",
    "result = np.dot(comp_trans, buis_trans)/(norm(comp_trans)*norm(buis_trans))\n",
    "print(1 - (spatial.distance.cosine(comp_trans, buis_trans)))\n",
    "print(1 - (spatial.distance.cosine(comp_trans, stock_trans)))\n",
    "print(1 - (spatial.distance.cosine(comp_trans, soft_trans)))\n",
    "print(1 - (spatial.distance.cosine(tech_trans, soft_trans)))\n",
    "print(1 - (spatial.distance.cosine(tech_trans, stock_trans)))\n",
    "fasttext_model.get_subwords('stock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "$ ./fasttext supervised\n",
    "Empty input or output path.\n",
    "\n",
    "The following arguments are mandatory:\n",
    "  -input              training file path\n",
    "  -output             output file path\n",
    "\n",
    "  The following arguments are optional:\n",
    "  -verbose            verbosity level [2]\n",
    "\n",
    "  The following arguments for the dictionary are optional:\n",
    "  -minCount           minimal number of word occurrences [1]\n",
    "  -minCountLabel      minimal number of label occurrences [0]\n",
    "  -wordNgrams         max length of word ngram [1]\n",
    "  -bucket             number of buckets [2000000]\n",
    "  -minn               min length of char ngram [0]\n",
    "  -maxn               max length of char ngram [0]\n",
    "  -t                  sampling threshold [0.0001]\n",
    "  -label              labels prefix [__label__]\n",
    "\n",
    "  The following arguments for training are optional:\n",
    "  -lr                 learning rate [0.1]\n",
    "  -lrUpdateRate       change the rate of updates for the learning rate [100]\n",
    "  -dim                size of word vectors [100]\n",
    "  -ws                 size of the context window [5]\n",
    "  -epoch              number of epochs [5]\n",
    "  -neg                number of negatives sampled [5]\n",
    "  -loss               loss function {ns, hs, softmax} [softmax]\n",
    "  -thread             number of threads [12]\n",
    "  -pretrainedVectors  pretrained word vectors for supervised learning []\n",
    "  -saveOutput         whether output params should be saved [0]\n",
    "\n",
    "  The following arguments for quantization are optional:\n",
    "  -cutoff             number of words and ngrams to retain [0]\n",
    "  -retrain            finetune embeddings if a cutoff is applied [0]\n",
    "  -qnorm              quantizing the norm separately [0]\n",
    "  -qout               quantizing the classifier [0]\n",
    "  -dsub               size of each sub-vector [2]\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "https://fasttext.cc/docs/en/python-module.html\n",
    "    get_dimension           # Get the dimension (size) of a lookup vector (hidden layer).\n",
    "                            # This is equivalent to `dim` property.\n",
    "    get_input_vector        # Given an index, get the corresponding vector of the Input Matrix.\n",
    "    get_input_matrix        # Get a copy of the full input matrix of a Model.\n",
    "    get_labels              # Get the entire list of labels of the dictionary\n",
    "                            # This is equivalent to `labels` property.\n",
    "    get_line                # Split a line of text into words and labels.\n",
    "    get_output_matrix       # Get a copy of the full output matrix of a Model.\n",
    "    get_sentence_vector     # Given a string, get a single vector represenation. This function\n",
    "                            # assumes to be given a single line of text. We split words on\n",
    "                            # whitespace (space, newline, tab, vertical tab) and the control\n",
    "                            # characters carriage return, formfeed and the null character.\n",
    "    get_subword_id          # Given a subword, return the index (within input matrix) it hashes to.\n",
    "    get_subwords            # Given a word, get the subwords and their indicies.\n",
    "    get_word_id             # Given a word, get the word id within the dictionary.\n",
    "    get_word_vector         # Get the vector representation of word.\n",
    "    get_words               # Get the entire list of words of the dictionary\n",
    "                            # This is equivalent to `words` property.\n",
    "    is_quantized            # whether the model has been quantized\n",
    "    predict                 # Given a string, get a list of labels and a list of corresponding probabilities.\n",
    "    quantize                # Quantize the model reducing the size of the model and it's memory footprint.\n",
    "    save_model              # Save the model to the given path\n",
    "    test                    # Evaluate supervised model using file given by path\n",
    "    test_label              # Return the precision and recall score for each label.\n",
    "\n",
    "    model.words         # equivalent to model.get_words()\n",
    "    model.labels        # equivalent to model.get_labels()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\"\n",
    "\n",
    "# Not really necessary to load data since train_supervised works directly off of a .txt file\n",
    "#data = np.load(os.getcwd() + 'news_data.npz')\n",
    "\n",
    "# Training is usually really fast\n",
    "print(\"Training model\")\n",
    "model = fasttext.train_supervised(input=\"dat_data_new_labels_cleaned.txt\", verbose=False, maxn=3, minn=3)\n",
    "\n",
    "mat = model.get_input_matrix()\n",
    "words = model.get_words()\n",
    "\n",
    "print(\"Mat is here\")\n",
    "print(mat.shape)\n",
    "\n",
    "print(\"Word length is\")\n",
    "print(len(words))\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "# Quickly cobbled-together test-set creator\n",
    "print(\"Creating test set\")\n",
    "txt = open('dat_data_new_labels_cleaned.txt', 'r')\n",
    "txt_arr = txt.read().split('\\n')\n",
    "\n",
    "tests = []\n",
    "labels = []\n",
    "for r, i in enumerate(txt_arr):\n",
    "    to_append = i.split(' ', 1)\n",
    "    tests.append(to_append[1])\n",
    "    labels.append(to_append[0])\n",
    "\n",
    "\n",
    "print('Predicting')\n",
    "su = 0\n",
    "for i, test in enumerate(tests):\n",
    "    predict_label = model.predict(test)[0][0]\n",
    "\n",
    "    if predict_label == labels[i]:\n",
    "        su += 1\n",
    "\n",
    "print(\"Total accuracy was \", su/len(tests))\n",
    "\"\"\"\n",
    "#print()\n",
    "\n",
    "\n",
    "# Next we'll try to obtain the vector representations of some simple words\n",
    "\n",
    "words = ['company', 'business', 'cat', 'software', 'microsoft']\n",
    "word_vectors = np.array([fasttext_model.get_word_vector(word) for word in words])\n",
    "print(word_vectors.shape)\n",
    "n = 2 # Number of principal components\n",
    "transformed_vectors = pca.components_[:n]@word_vectors.T\n",
    "transformed_vectors = transformed_vectors.T\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "origin = np.zeros((2,6)) # origin point\n",
    "\n",
    "\n",
    "print(transformed_vectors.shape)\n",
    "print(transformed_vectors)\n",
    "origin = np.zeros((2,6))\n",
    "\n",
    "max_dim = np.amax(abs(transformed_vectors))\n",
    "\n",
    "\n",
    "plt.quiver(np.zeros(6), np.zeros(6), transformed_vectors[:,0], transformed_vectors[:,1], angles='xy', scale_units='xy', color=['r','b','g', 'pink', 'cyan'])\n",
    "plt.legend([word for word in words])\n",
    "plt.grid(b=True, which='major') #<-- plot grid lines\n",
    "plt.xlim([-max_dim, max_dim]) #<-- set the x axis limits\n",
    "plt.ylim([-max_dim,max_dim]) #<-- set the y axis limits\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
