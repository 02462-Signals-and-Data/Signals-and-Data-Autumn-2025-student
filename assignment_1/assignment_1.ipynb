{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - CNN's and VGG16\n",
    "\n",
    "*In this assingment, you will further familiraize yourself with CNN's and how to implement them. For this particular example, we will ask you to implement the layer structure of VGG16, an old but fairly effective and simple CNN structure.*\n",
    "\n",
    "*Keep in mind, that while VGG16 and other CNN's you have implemented so far, only incoporate convolutions and pooling layers, many state-of-the-art models  use a variety of other techniques, such as skip connections (CITATION NEEDED), or self-attention (CITATION NEEDED) to get better results.*\n",
    "\n",
    "*As you write code for this assignment, try to keep in mind to write good code. That might sound vague, but just imagine that some other poor sod will have to read your code at some point, and easily readable, understandable code, will go a long way to making their life easier. However, this is not a coding course, so the main focus should of course be on the exercises themselves.*\n",
    "\n",
    "**Keep in mind, this assignment does not count towards your final grade in the course. When any of the exercises mention 'grading', it refers to commenting and correcting answers, not necessarily giving you a score which will reflect in your grade, so dw :)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Boilerplate start - you can ignore this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import PIL\n",
    "from torch import nn\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary\n",
    "\n",
    "# Check if you have cuda available, and use if you do\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Set a random seed for everything important\n",
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set a seed with a random integer, in this case, I choose my verymost favourite sequence of numbers\n",
    "seed_everything(sum([115, 107, 105, 98, 105, 100, 105, 32, 116, 111, 105, 108, 101, 116]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify dataset you wanna use\n",
    "def get_dataset(dataset_name, validation_size=0.1, transform=None, v=True):\n",
    "\n",
    "    if transform is None:\n",
    "        transform = ToTensor()\n",
    "\n",
    "    if dataset_name == 'cifar10':\n",
    "        train_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=ToTensor())\n",
    "        test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "        # Purely for our convenience - Mapping from cifar labels to human readable classes\n",
    "        cifar10_classes = {\n",
    "            0: 'airplane',\n",
    "            1: 'automobile',\n",
    "            2: 'bird',\n",
    "            3: 'cat',\n",
    "            4: 'deer',\n",
    "            5: 'dog',\n",
    "            6: 'frog',\n",
    "            7: 'horse',\n",
    "            8: 'ship',\n",
    "            9: 'truck'\n",
    "        }\n",
    "\n",
    "    elif dataset_name == 'mnist':\n",
    "        train_set = datasets.MNIST(root='./data', train=True, download=True, transform=ToTensor())\n",
    "        test_set = datasets.MNIST(root='./data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "    elif dataset_name == 'imagenette':\n",
    "        download = not os.path.exists('./data/imagenette2')\n",
    "\n",
    "        # Specific transform in the case we use imagenette\n",
    "        imagenette_transform = transforms.Compose([\n",
    "            transforms.Resize(256),        # Resize to 256x256\n",
    "            transforms.RandomCrop(224),    # Crop the center to 224x224\n",
    "            transforms.ToTensor(),         # Convert to tensor\n",
    "            transforms.Normalize(mean=[0.4650, 0.4553, 0.4258], std=[0.2439, 0.2375, 0.2457]) # Normalize each image, numbers because of function courtesy of chatgpt\n",
    "        ])\n",
    "        train_set = datasets.Imagenette(root='./data', split='train', download=download, size='full', transform=imagenette_transform)\n",
    "        test_set = datasets.Imagenette(root='./data', split='val', download=download, size='full', transform=imagenette_transform)\n",
    "    \n",
    "    # If we want a validation set of a given size, take it from train set\n",
    "    if validation_size is not None:\n",
    "        # These will both be of the torch.utils.data.Subset type (not the Dataset type), and are basically just mappings of indices\n",
    "        # This does not matter when we make Dataloaders of them, however\n",
    "        if dataset_name != 'imagenette':\n",
    "            train_set, validation_set = torch.utils.data.random_split(train_set, [1-validation_size, validation_size])\n",
    "\n",
    "        # In the case of imagenette, the 'test set' is already a pretty big validation set, so we'll use that to create the test set instead\n",
    "        else:\n",
    "            validation_set, test_set = torch.utils.data.random_split(test_set, [validation_size, 1-validation_size])\n",
    "\n",
    "    if v:\n",
    "        print(f\"There are {len(train_set)} examples in the training set\")\n",
    "        print(f\"There are {len(test_set)} examples in the test set \\n\")\n",
    "\n",
    "        print(f\"Image shape is: {train_set[0][0].shape}, label example is {train_set[0][1]}\")\n",
    "\n",
    "    return train_set, validation_set, test_set\n",
    "\n",
    "# collate function just to cast to device, same as in week_3 exercises\n",
    "def collate_fn(batch):\n",
    "    return tuple(x_.to(device) for x_ in default_collate(batch))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-implementation questions\n",
    "\n",
    "*These questions are meant to test your general knowledge of CNN's*\n",
    "\n",
    "### Exercise 1.1\n",
    "\n",
    "**1. What is the reason we add MaxPooling or AveragePooling in CNN's?**\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "\n",
    "**2. Say a network comes with a list of class probabilities:** $\\hat{p}_1, \\hat{p}_2, \\dots \\hat{p}_N$ **when is the cross-entropy in regards to the *true* class probabilities:** $p_1, p_2, \\dots p_N$ **maximized?**\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "**3. In the [VGG paper](https://arxiv.org/pdf/1409.1556), last paragraph of 'training', page 4, they mention images being randomly cropped after being rescaled. Why do you think they crop images only *after* rescaling them?**\n",
    "\n",
    "**4. After this, they mention \"further augmenting the dataset\" by random horizontal flipping and random RGB color shift. Why do you think they do this?**\n",
    "\n",
    "**5. Why do you think they do not randomly translate images? (Translate being to move images left, right, up, down)**\n",
    "\n",
    "**6. Which of the following classification tasks do you think is more difficult for a machine learning model, and why?**\n",
    "\n",
    "- Telling German Shepherds (Sch√¶ferhunde) from Labradors\n",
    "- Telling dogs from cats\n",
    "- Telling horses from cars from totem poles from chainsaws\n",
    "\n",
    "**7. In real life, you often find that neural networks aren't used \"for everything\", older and often more simple models like random forest and linear regression still dominate a lot of fields.**\n",
    "\n",
    "- Reason a bit about why this is the case\n",
    "\n",
    "**8. When we sample from our dataloader, we sample in batches, why is this? What would be the alternatives to sampling in batches, and what impact would that have?**\n",
    "\n",
    "**9. The VGG16-D conv layers all use the same kernel size. Come up with reasons for why you would use bigger/smaller kernel sizes**\n",
    "\n",
    "**\\*10. The \"new kid on the block' (relatively speaking) in NLP (Natural Language processing), is self-attention. Basically this is letting each word/token relate to each other word/token by a specific 'attention' value, vaguely showing how much they relate to one another.**\n",
    "\n",
    "- Would there be any problems in doing this for image processing by simply letting each pixel relate to each other pixel, so we can get spatial information that way instead?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boilerplate end - Your implementation work begins here:\n",
    "\n",
    "*Below, you are given a working example of a CNN, not much different from the one in the exercises of week 3. Your job is to complete the implementation questions below. *\n",
    "\n",
    "*You do not need to do all the exercises below, or even do them in order, we will obviously only grade the ones you have done, however. Please just mark completed exercises with an X as shown below, so we will know what to look for when grading your assignment. You can add as much text below each question as you want to either argue for your choice of implementation, discuss your results, or ask us questions, we will consider this when grading the assignment.*\n",
    "\n",
    "**X 0. This marks a question which has been completed**\n",
    "\n",
    "*For your convenience, we reccommend implementing two models: One bigger for the VGG16-D exercises, meant to be used only with images from the Imagenette dataset, and one smaller, which can also take the other datasets. The model already implemented below should fill the role of the latter.*\n",
    "\n",
    "*Finally, if you're not able to train the VGG16-D model because it is too big, you can also load the weights of the model using the funciton implemented for exactly that. We do, however, reccommend training it from scratch yourself, if possible.*\n",
    "\n",
    "______________________________________________________________________________________________\n",
    "\n",
    "\n",
    "\n",
    "**1. Implement the layer structure of VGG16-D by following either this [Medium article](https://medium.com/@mygreatlearning/everything-you-need-to-know-about-vgg16-7315defb5918) (fairly easy), or the [official paper](https://arxiv.org/pdf/1409.1556) (slightly harder) (Note: This layer structure is meant to be used with 224x224 sized images, only the  imagenette dataset in this notebook has this)**\n",
    "\n",
    "**2. Figure out, and implement the type, and exact settings of the optimizer the original VGG16-D implementation used**\n",
    "\n",
    "**3.** \n",
    "- **Can you make the VGG16-D model overfit to the imagenette dataset? If not, what about another dataset?**\n",
    "- **Can you change the amount of dropout to increase or decrease the rate of overfitting?**\n",
    "- **Can you make the smaller model overfit to any of its datasets? Is it harder or easier? Explain your answer**\n",
    "\n",
    "**4. Try to improve the test accuracy of either of your models by changing some of they hyperparameters. To make it easier, try to keep detailed results of your experimental setups and your preliminary results. Argue for your changes. Examples of possible changes are shown below:**:\n",
    "- **Add more/fewer kernels**\n",
    "- **Add more/less dropout**\n",
    "- **Add [BatchNorm](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)**\n",
    "- **Change the intial transform when loading the data to make larger/smaller images**\n",
    "\n",
    "**5. Change the model (or any other code in the whole script) to make either training, inference, or both, as quick as possible, while still keeping a reasonable test accuracy. You can time training and inference of your network by adding the timeit=True argument. What did you do to achieve this?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9469 examples in the training set\n",
      "There are 3532 examples in the test set \n",
      "\n",
      "Image shape is: torch.Size([3, 224, 224]), label example is 0\n"
     ]
    }
   ],
   "source": [
    "# Compose custom transform\n",
    "# transform = transforms.Compose(\n",
    "#     [transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Get data\n",
    "dataset_name = 'imagenette'\n",
    "train_set, validation_set, test_set = get_dataset(dataset_name, validation_size=0.1)\n",
    "\n",
    "# For VGG16, we use channel-wise mean and STD, not pixelwise\n",
    "\n",
    "\n",
    "# Make dataloaders\n",
    "batch_size=16\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "validation_dataloader = torch.utils.data.DataLoader(validation_set, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(torch.nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=1, features_fore_linear=25088, dataset=None, lr=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        conv_stride = 1\n",
    "        pool_stride = 2\n",
    "        conv_kernel = (3,3)\n",
    "        pool_kernel = (2,2)\n",
    "        dropout_probs = 0.5\n",
    "        optim_momentum = 0.9\n",
    "        weight_decay = 5e-4\n",
    "        learning_rate = 5e-2\n",
    "\n",
    "        # Define layers as a torch.nn.Sequential object\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=conv_kernel, stride=conv_stride, padding='same'), # dim = in\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=conv_kernel, stride=conv_stride, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel, stride=pool_stride),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=conv_kernel, stride=conv_stride, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=conv_kernel, stride=conv_stride, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel, stride=pool_stride),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=conv_kernel, stride=conv_stride, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=conv_kernel, stride=conv_stride, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=conv_kernel, stride=conv_stride, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel, stride=pool_stride),\n",
    "\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=conv_kernel, stride=conv_stride, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=conv_kernel, stride=conv_stride, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=conv_kernel, stride=conv_stride, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel, stride=pool_stride),\n",
    "\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=conv_kernel, stride=conv_stride, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=conv_kernel, stride=conv_stride, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=conv_kernel, stride=conv_stride, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel, stride=pool_stride),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=features_fore_linear, out_features=4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_probs),\n",
    "            nn.Linear(in_features=4096, out_features=4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_probs),\n",
    "            nn.Linear(in_features=4096, out_features=num_classes)\n",
    "        ).to(device)\n",
    "        \n",
    "        # In the paper, they mention updating towards the 'multinomial logistic regression objective'\n",
    "        # As can be read in Bishop p. 159, taking the logarithm of this equates to the cross-entropy loss function.\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Optimizer - For now just set to Adam to test the implementation\n",
    "        # self.optim = torch.optim.Adam(self.layers.parameters(), lr=learning_rate)\n",
    "        self.optim = torch.optim.SGD(self.layers.parameters(), lr=learning_rate, momentum=optim_momentum, weight_decay=weight_decay)\n",
    "\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    def train(self, train_dataloader, epochs=1, val_dataloader=None):\n",
    "        \n",
    "        # Call .train() on self to turn on dropout\n",
    "        self.train()\n",
    "\n",
    "        # To hold accuracy during training and testing\n",
    "        train_accs = []\n",
    "        test_accs = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            epoch_acc = 0\n",
    "\n",
    "            for inputs, targets in tqdm(train_dataloader):\n",
    "                logits = self.forward(inputs)\n",
    "                loss = self.criterion(logits, targets)\n",
    "                loss.backward()\n",
    "\n",
    "                self.optim.step()\n",
    "                self.optim.zero_grad()\n",
    "\n",
    "                # Keep track of training accuracy\n",
    "                epoch_acc += (torch.argmax(logits, dim=1) == targets).sum().item()\n",
    "\n",
    "            train_accs.append(epoch_acc / len(train_dataloader.dataset))\n",
    "\n",
    "            # If val_dataloader, evaluate after each epoch\n",
    "            if val_dataloader is not None:\n",
    "                # Turn off dropout for testing\n",
    "                self.test()\n",
    "                acc = self.eval(val_dataloader)\n",
    "                test_accs.append(acc)\n",
    "                print(f\"Epoch {epoch} validation accuracy: {acc}\")\n",
    "                # turn on dropout after being done\n",
    "                self.train()\n",
    "        \n",
    "        return train_accs, test_accs\n",
    "\n",
    "    def eval(self, test_dataloader):\n",
    "        \n",
    "        self.test()\n",
    "        total_acc = 0\n",
    "\n",
    "        for input_batch, label_batch in test_dataloader:\n",
    "            logits = self(input_batch)\n",
    "            classifications = torch.argmax(logits, dim=1)\n",
    "            total_acc += (classifications == label_batch).sum().item()\n",
    "\n",
    "        total_acc = total_acc / len(test_dataloader.dataset)\n",
    "\n",
    "        return total_acc\n",
    "\n",
    "    def predict(self, img_path):\n",
    "        # TODO: REMOVE .CONVERT\n",
    "        img = PIL.Image.open(img_path)\n",
    "        img = self.dataset.dataset.transform(img)\n",
    "        classification = torch.argmax(self(img.unsqueeze(dim=0)), dim=1)\n",
    "        return img, classification\n",
    "    \n",
    "    def predict_random(self, num_predictions=16):\n",
    "        \"\"\"\n",
    "        Plot random images from own given dataset\n",
    "        \"\"\"\n",
    "        random_indices = np.random.choice(len(self.dataset)-1, num_predictions, replace=False)\n",
    "        classifcations = []\n",
    "        labels = []\n",
    "        images = []\n",
    "        for idx in random_indices:\n",
    "            img, label = self.dataset.__getitem__(idx)\n",
    "\n",
    "            classifcation = torch.argmax(self(img.unsqueeze(dim=0)), dim=1)\n",
    "\n",
    "            classifcations.append(classifcation)\n",
    "            labels.append(label)\n",
    "            images.append(img)\n",
    "\n",
    "        return classifcations, labels, images\n",
    "\n",
    "# def plot_images_and_classifications(images, classifcations):\n",
    "\n",
    "def get_vgg_weights():\n",
    "    vgg = VGG16()\n",
    "    temp = torchvision.models.vgg16(pretrained=True)\n",
    "    vgg.load_state_dict(temp.state_dict())\n",
    "    return vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 224\n",
      "112\n",
      "56\n",
      "28\n",
      "14\n",
      "7\n",
      "25088\n",
      "([tensor([9]), tensor([7]), tensor([7]), tensor([7]), tensor([9]), tensor([9]), tensor([7]), tensor([4]), tensor([9]), tensor([9]), tensor([9]), tensor([9]), tensor([9]), tensor([9]), tensor([9]), tensor([7])], [8, 4, 8, 2, 4, 1, 4, 9, 2, 1, 4, 3, 1, 8, 4, 7])\n",
      "(tensor([[[-1.9065, -1.9065, -1.9065,  ..., -0.6685, -0.6685, -0.6685],\n",
      "         [-1.9065, -1.9065, -1.9065,  ..., -0.6685, -0.6685, -0.6685],\n",
      "         [-1.9065, -1.9065, -1.9065,  ..., -0.6685, -0.6685, -0.6685],\n",
      "         ...,\n",
      "         [-0.8453, -0.8132, -0.7810,  ..., -1.9065, -1.9065, -1.9065],\n",
      "         [-0.8936, -0.8614, -0.8132,  ..., -1.9065, -1.9065, -1.9065],\n",
      "         [-0.9579, -0.9096, -0.8614,  ..., -1.9065, -1.9065, -1.9065]],\n",
      "\n",
      "        [[-1.9171, -1.9171, -1.9171,  ...,  0.1965,  0.1965,  0.1965],\n",
      "         [-1.9171, -1.9171, -1.9171,  ...,  0.1965,  0.1965,  0.1965],\n",
      "         [-1.9171, -1.9171, -1.9171,  ...,  0.1965,  0.1965,  0.1965],\n",
      "         ...,\n",
      "         [-0.1503, -0.0677, -0.0182,  ..., -1.9171, -1.9171, -1.9171],\n",
      "         [-0.2659, -0.1668, -0.1007,  ..., -1.9171, -1.9171, -1.9171],\n",
      "         [-0.3814, -0.2824, -0.1833,  ..., -1.9171, -1.9171, -1.9171]],\n",
      "\n",
      "        [[-1.7330, -1.7330, -1.7330,  ..., -1.1584, -1.1584, -1.1584],\n",
      "         [-1.7330, -1.7330, -1.7330,  ..., -1.1584, -1.1584, -1.1584],\n",
      "         [-1.7330, -1.7330, -1.7330,  ..., -1.1584, -1.1584, -1.1584],\n",
      "         ...,\n",
      "         [-1.2063, -1.1903, -1.1903,  ..., -1.7330, -1.7330, -1.7330],\n",
      "         [-1.2223, -1.2063, -1.2063,  ..., -1.7330, -1.7330, -1.7330],\n",
      "         [-1.2382, -1.2223, -1.2223,  ..., -1.7330, -1.7330, -1.7330]]]), tensor([9]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_106965/3194574221.py:31: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  plt.legend()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "in_channels = next(iter(train_dataloader))[0].shape[1]\n",
    "in_width_height = next(iter(train_dataloader))[0].shape[-1]\n",
    "\n",
    "print(in_channels, in_width_height)\n",
    "\n",
    "# Make a dummy model to find out the size before the first linear layer\n",
    "CNN_model = VGG16(num_classes=10, in_channels=in_channels, lr=0.001)\n",
    "features_fore_linear = get_dim_before_first_linear(CNN_model.layers, in_width_height, in_channels, brain=False)\n",
    "\n",
    "print(features_fore_linear)\n",
    "\n",
    "# Now make true model when we know how many features we have before the first linear layer\n",
    "CNN_model = VGG16(num_classes=10, in_channels=in_channels, features_fore_linear=features_fore_linear, lr=0.001, dataset=test_set) \n",
    "\n",
    "train_epochs = 5\n",
    "# train_accs, test_accs = CNN_model.train(train_dataloader, epochs=train_epochs,  val_dataloader=test_dataloader)\n",
    "# CNN_model.eval(test_dataloader)\n",
    "\n",
    "# plot train and test accuracies\n",
    "# plt.plot(range(train_epochs), train_accs, label='Train')\n",
    "# plt.plot(range(train_epochs), test_accs, label='Test')\n",
    "\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train and Test Accuracy over epochs, CNN model')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# TODO: ADD QUESTION: WHY IS TRAIN ACCURACY MUCH LOWER THAN TEST ACCURACY FOR THE FIRST EPOCH?\n",
    "\n",
    "\n",
    "# model.forward_checker(torch.zeros(16, 3, 64, 64))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util functions\n",
    "\n",
    "def compute_channel_stats(dataset, batch_size=32):\n",
    "    \"\"\"\n",
    "    Compute the channel-wise mean and standard deviation of all images in a dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset (torch.utils.data.Dataset): A dataset object that returns (image, label) pairs.\n",
    "        batch_size (int): Batch size for processing the dataset.\n",
    "    \n",
    "    Returns:\n",
    "        mean (tensor): A tensor containing the channel-wise mean.\n",
    "        std (tensor): A tensor containing the channel-wise standard deviation.\n",
    "    \"\"\"\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    mean_sum = 0.0\n",
    "    std_sum = 0.0\n",
    "    total_images = 0\n",
    "\n",
    "    for images, _ in dataloader:\n",
    "        images = images.view(images.size(0), images.size(1), -1)\n",
    "        \n",
    "        total_images += images.size(0)\n",
    "        \n",
    "        mean_sum += images.mean(dim=2).sum(dim=0)\n",
    "        std_sum += images.var(dim=2, unbiased=False).sum(dim=0)\n",
    "    \n",
    "    mean = mean_sum / total_images\n",
    "    std = torch.sqrt(std_sum / total_images)\n",
    "    \n",
    "    return mean, std\n",
    "\n",
    "def get_dim_before_first_linear(layers, in_dim, in_channels, brain=False):\n",
    "    \"\"\"\n",
    "    Assume square in dimensions, square kernels, cuz I'm lazy\n",
    "    Also assume kernel numbers and channels match up, because that's trivial enough\n",
    "    \"\"\"\n",
    "\n",
    "    current_dim = in_dim\n",
    "    current_channels = in_channels\n",
    "    for layer in layers:\n",
    "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.MaxPool2d):\n",
    "            # If the layer padding is same we do not need to change the dimension of the input...\n",
    "            if layer.padding == 'same':\n",
    "                if isinstance(layer, nn.Conv2d):\n",
    "                    current_channels = layer.out_channels\n",
    "                continue\n",
    "            vals = {\n",
    "                'kernel_size': layer.kernel_size if isinstance(layer.kernel_size, int) else layer.kernel_size[0],\n",
    "                'stride': layer.stride if isinstance(layer.stride, int) else layer.stride[0],\n",
    "                'padding': layer.padding if isinstance(layer.padding, int) else layer.padding[0],\n",
    "                'dilation': layer.dilation if isinstance(layer.dilation, int) else layer.dilation[0]\n",
    "            }\n",
    "            current_dim = (current_dim + 2*vals['padding'] - vals['dilation']*(vals['kernel_size'])) // vals['stride'] + 1\n",
    "            print(current_dim)\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            current_channels = layer.out_channels\n",
    "\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            if brain:\n",
    "                return current_dim, current_channels\n",
    "            else:\n",
    "                return current_dim * current_dim * current_channels\n",
    "        \n",
    "    raise ValueError(\"No linear layer found in layers! Why are you even asking me?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sign-dat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
