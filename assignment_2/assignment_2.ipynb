{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Assignment 2 EEG Data Analysis\n",
    "\n",
    "*This assignment is an exploration into a dataset of EEG data on patients. The data contains EEG measurements for people who were first asked to keep their eyes open (oa) and then closed (oc) for around 4-5 minutes.*\n",
    "\n",
    "*The main aim is to give you an application of the Discrete Fourier Transform on real world data. As this step can be done in a single function a lot of the time spent on this exercise will be on visually inspecting the data and trying different classifiers on the data you have produced. If you are lucky enough to draw this assignment for your exam you should be prepared to answer mainly theoretical questions about the Fourier Transform as covered in the exercises and lectures from week 5&6.*\n",
    "\n",
    "\n",
    "Apart from the usual libraries you have been using in this course, you will have to install mne via pip or conda."
   ],
   "id": "44f4f4c935c06e5"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from os.path import join\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "path_data = \"egg_data_assignment_2\"\n",
    "data_oa = []\n",
    "data_oc = []\n",
    "sample_rate = 250   # DO NOT CHANGE THIS\n",
    "n_subjects = -1\n",
    "\n",
    "for folder in tqdm(os.listdir(path_data)[:n_subjects], desc=\"Loading data\"):\n",
    "    for filename in os.listdir(join(path_data, folder)):\n",
    "        df = pd.read_csv(join(path_data, folder, filename), sep=\",\", index_col=0)\n",
    "        if \"oa\" in filename:\n",
    "            data_oa.append(df.values)\n",
    "        else:\n",
    "            data_oc.append(df.values)"
   ],
   "id": "72eb7f9b0506a7d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(F\"Loaded data from {len(data_oa)} patients\")"
   ],
   "id": "e07fa0fa0fdca4b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "*Now we want to center all the data around the same value (0), in eeg data analysis terms this is called referencing. Here we will apply a simple average referencing*\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "**1. Implement the following function:**\n"
   ],
   "id": "624394fe2adc8827"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def set_reference(eeg_data, ref_channels):\n",
    "    \"\"\"Take the mean of the reference channels and subtract that from the data\"\"\"\n",
    "    eeg_masked = eeg_data[ref_channels]\n",
    "    ref_signal = np.mean(?, axis=?)\n",
    "    re_referenced_data = ?\n",
    "    return re_referenced_data"
   ],
   "id": "c6b5c739d7cc1eff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "*We will reference each patient's data individually:*"
   ],
   "id": "dcd5cfbece3a0749"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i in range(len(data_oa)):\n",
    "    data_oc[i] = set_reference(data_oc[i], np.arange(20))\n",
    "    data_oa[i] = set_reference(data_oa[i], np.arange(20))"
   ],
   "id": "8c499f3581366b2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Exercise 1\n",
    "\n",
    "**2. Using the following plotting function, have a look at the data and comment on the quality of it:**\n"
   ],
   "id": "963e6a354bbb9740"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_eeg_mne_style(eeg_data, ch_names, sampling_rate):\n",
    "    \"\"\"\n",
    "    Plot EEG data in an MNE-style plot for each channel.\n",
    "\n",
    "    Parameters:\n",
    "    eeg_data : np.ndarray\n",
    "        The EEG data with shape (n_channels, n_points).\n",
    "    ch_names : list of str\n",
    "        List of channel names (must be of length n_channels).\n",
    "    sampling_rate : float\n",
    "        The sampling rate of the EEG data (in Hz).\n",
    "    \"\"\"    \n",
    "    # Create MNE info structure with channel names and sampling rate\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sampling_rate, ch_types='eeg')\n",
    "    \n",
    "    # Create MNE RawArray object with the data\n",
    "    raw = mne.io.RawArray(eeg_data, info)\n",
    "    \n",
    "    # Plot the EEG data with MNE's built-in plot function\n",
    "    raw.plot(scalings='auto', show=True, block=True)\n",
    "    plt.show()"
   ],
   "id": "f93c0c344b054dfc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "channel_names = np.arange(1,21,1).astype(str).tolist()\n",
    "plot_eeg_mne_style(data_oa[4], ch_names=channel_names, sampling_rate=sample_rate)"
   ],
   "id": "a6ba5f9c31c30d0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 2 Discrete Fourier Transform\n",
    "*This exercise should seem very familiar by now and is included here as this will be the main focus if you draw this assignment at the exam*\n",
    "\n",
    "\n",
    "*You are given the following information about a sampled signal:\n",
    "\n",
    "- Sampling rate: $f_s = 6$\n",
    "- Duration: $1s$\n",
    "- Signal values $x(n) = [ 0,  1,  0, -1, 0, -1]$\n",
    "\n",
    "**1. Using the formula for fourier coefficients: $c_k = \\sum_{n = 0}^{N-1} x(n) e^{-i 2\\pi \\frac{kn}{N}}$, calculate the fourier coefficient for the signal $x(n)$ corresponding to $k = 1$**\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "**2. Calculate the frequency that the fourier coefficient for $k = 1$ corresponds to**\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "**3. Calculate the amplitude of the frequency that corresponds to the fourier coefficient for $k = 1$**\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "**4. Given the signal values for the above signal, what is the original analog signal?**\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "**$\\star$ 5 Perform the same 4 exercises above but for $k = 3$ and for following values:**\n",
    "\n",
    "- Sampling rate: $f_s = 4$\n",
    "- Duration: $2s$\n",
    "- Signal values $x(n) = [ 0,     2.828, -4,     2.828,  0,    -2.828,  4,    -2.828]$\n",
    "\n",
    "\n",
    "$\\dots$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8c94476999d2373"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Exercise 3 Frequency domain\n",
    "\n",
    "**1. Complete the following function which takes a signal, computes the spectrogram and allow the user to filter the spectrogram based on a minimum and maximum frequency:**\n",
    "\n",
    "*We want the bounds to be inclusive*\n"
   ],
   "id": "f31319b3789c3c8d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compute_power_spectrum(signal, sampling_rate, min_freq=None, max_freq=None):\n",
    "    \"\"\"\n",
    "    Computes the power spectrum for each channel in the signal.\n",
    "\n",
    "    Parameters:\n",
    "    signal : np.ndarray\n",
    "        Input signal with shape (n_channels, n_points)\n",
    "    sampling_rate : float\n",
    "        The sampling rate of the signal (in Hz).\n",
    "\n",
    "    Returns:\n",
    "    freqs : np.ndarray\n",
    "        The frequencies corresponding to the power spectrum.\n",
    "    power_spectrum : np.ndarray\n",
    "        Power spectrum of each channel with shape (n_channels, n_points//2).\n",
    "    \"\"\"\n",
    "    n_channels, n_points = signal.shape\n",
    "    \n",
    "    # Perform the FFT for each channel\n",
    "    fft_vals = np.fft.rfft(?, axis=1)\n",
    "    \n",
    "    # Compute the power spectrum (squared magnitude of the FFT)\n",
    "    power_spectrum = ?\n",
    "    \n",
    "    # Compute the corresponding frequencies\n",
    "    freqs = np.fft.rfftfreq(?)\n",
    "    \n",
    "    if min_freq is not None:\n",
    "        power_spectrum = power_spectrum[?]\n",
    "        freqs = freqs[?]\n",
    "\n",
    "    if max_freq is not None:\n",
    "        power_spectrum = power_spectrum[?]\n",
    "        freqs = freqs[?]\n",
    "    \n",
    "    return freqs, power_spectrum"
   ],
   "id": "8a510404cd8586bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_power_spectrum_overlay(freqs, power_spectrum, ch_names, show=False, title=\"\"):\n",
    "    \"\"\"\n",
    "    Plot the power spectrum for all channels on the same plot.\n",
    "\n",
    "    Parameters:\n",
    "    eeg_data : np.ndarray\n",
    "        The EEG data with shape (n_channels, n_points).\n",
    "    sampling_rate : float\n",
    "        The sampling rate of the EEG data (in Hz).\n",
    "    ch_names : list of str\n",
    "        List of channel names.\n",
    "    \"\"\"\n",
    "    colors = cm.viridis(np.linspace(0, 1, len(ch_names)))\n",
    "\n",
    "    for i, ch_name in enumerate(ch_names):\n",
    "        plt.plot(freqs, power_spectrum[i], color=colors[i], label=ch_name)\n",
    "\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.ylabel('Power')\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.grid(True)\n",
    "    plt.title('Power Spectrum for EEG Channels' if not title else title)\n",
    "    plt.tight_layout()\n",
    "    if show:\n",
    "        plt.show()"
   ],
   "id": "1aa142f3108f4b19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "*We have been advised by Ivana (Human Cognition lecturer and EEG researcher), that we should take a look at the frequencies between 1 and 40 Hz*"
   ],
   "id": "1cc2b359a450d951"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "segments_to_plot = [0, 3, 5] # Feel free to modify\n",
    "min_freq=?\n",
    "max_freq=?\n",
    "for segment in segments_to_plot:\n",
    "    plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Compute power spectra for both conditions\n",
    "    freqs_oc, power_spectrum_oc = compute_power_spectrum(data_oc[segment], sampling_rate=sample_rate, min_freq=min_freq, max_freq=max_freq)\n",
    "    freqs_oa, power_spectrum_oa = compute_power_spectrum(data_oa[segment], sampling_rate=sample_rate, min_freq=min_freq, max_freq=max_freq)\n",
    "    \n",
    "    # Determine the y-limit based on the maximum value of both power spectra\n",
    "    y_max = max(power_spectrum_oc.max(), power_spectrum_oa.max())\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plot_power_spectrum_overlay(freqs_oc, power_spectrum_oc, channel_names, title=f\"Power spectrum for Eyes Closed (OC)\")\n",
    "    plt.ylim([0, y_max + y_max / 10])\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plot_power_spectrum_overlay(freqs_oa, power_spectrum_oa, channel_names, title=f\"Power spectrum for Eyes Open (OA)\")\n",
    "    plt.ylim([0, y_max + y_max / 10])\n",
    "    plt.show()"
   ],
   "id": "866e991737b1e773",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**2. Using the loop above take a look at the spectrograms for eyes open and eyes closed.** \n",
    "- Is there a noticeable difference between the two? How could we use this difference for classification?\n",
    "- Discuss whether the conditions of our fft are sufficient to avoid spectral leakage\n",
    " "
   ],
   "id": "2cc6b501ca8a5201"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "dfb38966b8f7c825"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**3. The channels at index 17 and 18 are known to be correlated to have open vs closed eyes, plot these:**"
   ],
   "id": "b2e9d81173f615e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "optical_channel_slice = slice(?)\n",
    "data_oc_optical_channels = [arr[optical_channel_slice] for arr in data_oc]\n",
    "data_oa_optical_channels = [arr[optical_channel_slice] for arr in data_oa]"
   ],
   "id": "a0dee92ef5b2699",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "segments_to_plot = [0, 3, 5]\n",
    "for segment in segments_to_plot:\n",
    "    plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Compute power spectra for both conditions\n",
    "    freqs_oc, power_spectrum_oc = compute_power_spectrum(data_oc_optical_channels[segment], sampling_rate=sample_rate, min_freq=min_freq, max_freq=max_freq)\n",
    "    freqs_oa, power_spectrum_oa = compute_power_spectrum(data_oa_optical_channels[segment], sampling_rate=sample_rate, min_freq=min_freq, max_freq=max_freq)\n",
    "    \n",
    "    # Determine the y-limit based on the maximum value of both power spectra\n",
    "    y_max = max(power_spectrum_oc.max(), power_spectrum_oa.max())\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plot_power_spectrum_overlay(freqs_oc, power_spectrum_oc, channel_names[optical_channel_slice], title=f\"Power spectrum for Eyes Closed (OC)\")\n",
    "    plt.ylim([0, y_max + y_max / 10])\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plot_power_spectrum_overlay(freqs_oa, power_spectrum_oa, channel_names[optical_channel_slice], title=f\"Power spectrum for Eyes Open (OA)\")\n",
    "    plt.ylim([0, y_max + y_max / 10])\n",
    "    plt.show()"
   ],
   "id": "204d7550f9fbb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**4. Now get the power spectra from the range that seems to be the most suited for classifying the difference between OC and OA based on the visual inspection you did above:**"
   ],
   "id": "24c7417b4d7d6136"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "min_freq = ?\n",
    "max_freq = ?\n",
    "power_spectra_oc = [compute_power_spectrum(arr, sampling_rate=sample_rate, min_freq=min_freq, max_freq=max_freq) for arr in data_oc_optical_channels]\n",
    "power_spectra_oa = [compute_power_spectrum(arr, sampling_rate=sample_rate, min_freq=min_freq, max_freq=max_freq) for arr\n",
    "                    in data_oa_optical_channels]"
   ],
   "id": "8c21a3a712fcdbda",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "*Let's plot this smaller spectrum*"
   ],
   "id": "40d377e2e3f31458"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_plots = 3\n",
    "for (freqs_oc, spectrum_oc), (freqs_oa, spectrum_oa) in zip(power_spectra_oc, power_spectra_oa[:n_plots]):\n",
    "    plt.subplots(1, 2, figsize=(15, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plot_power_spectrum_overlay(freqs_oc, spectrum_oc, ch_names=[17, 18], title=f\"Power spectrum for Eyes Closed (OC)\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plot_power_spectrum_overlay(freqs_oa, spectrum_oa, ch_names=[17, 18], title=f\"Power spectrum for Eyes Open (OA)\")\n",
    "    plt.ylim(spectrum_oc.min(), spectrum_oa.max())\n",
    "    plt.show()"
   ],
   "id": "66ee93e5c34964f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**5. Now let's compute the energies in this range and plot them**\n",
    "\n",
    "\n",
    "*Hint: recall that the energy of a signal is given by $E = \\sum_{n=0}^N f(n)^2$*"
   ],
   "id": "f56350331394ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "alphas_oc = []\n",
    "alphas_oa = []\n",
    "for (_, spectrum_oc), (_, spectrum_oa) in zip(power_spectra_oc, power_spectra_oa):\n",
    "    alpha_oc = ?\n",
    "    alpha_oa = ?\n",
    "    \n",
    "    alphas_oc.append(alpha_oc)\n",
    "    alphas_oa.append(alpha_oa)\n",
    "alphas_oc = np.array(alphas_oc)\n",
    "alphas_oa = np.array(alphas_oa)"
   ],
   "id": "143cb7580a1d651b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_alphas(alphas_oc, alphas_oa, show=False, s = 10):\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.scatter(range(len(alphas_oc)), alphas_oc, label='OC', color='blue', s=s) \n",
    "    plt.scatter(range(len(alphas_oa)), alphas_oa, label='OA', color='orange', s=s)\n",
    "    # The following line makes a line which you can change to illustrate a decision boundary:\n",
    "    plt.title('OA and OC energy of alpha frequencies compared')\n",
    "    plt.xlabel('Subject')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if show:\n",
    "        plt.show()"
   ],
   "id": "540eaab0fef02a09",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_alphas(alphas_oc, alphas_oa, show=True)"
   ],
   "id": "3172674ee0f16a6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**6. Now that you have processed the data using the dft and inspected the energies of the two different cases, discuss how well you believe this data could be used to create a classifier that classifies whether a person has their eyes open or close:**\n",
    "\n"
   ],
   "id": "1b644d12b65b6af8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Exercise 4 Classification\n",
    "\n",
    "*This exercise is here to ties what you learned in weeks 1-3 together with signals processing. Specifically we will implement a CNN to do classification on the EEG data. Here you can spend as much time and effort as you wish refining the classification or coming up with other techniques, but be warned EEG data is notoriously noisy.*"
   ],
   "id": "68051f2c9e07947"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def cross_validation_split(data_oc, data_oa, hold_out_val, i, device, as_arrays=False):\n",
    "    \"\"\"\n",
    "    Performs cross-validation split for one of time, frequency, or alpha data.\n",
    "    \n",
    "    Args:\n",
    "        data_oc (list or array): Optical channels (oc) data.\n",
    "        data_oa (list or array): Optical amplifiers (oa) data.\n",
    "        hold_out_val (int): Number of elements to hold out in each fold.\n",
    "        i (int): Current fold index.\n",
    "    \n",
    "    Returns:\n",
    "        train_current (list or array): Training set for current fold.\n",
    "        val_current (list or array): Validation set for current fold.\n",
    "    \"\"\"    \n",
    "    # Define slices for validation and training sets\n",
    "    slice_val = slice(i * hold_out_val, (i + 1) * hold_out_val)\n",
    "    before_slice = slice(0, i * hold_out_val)\n",
    "    after_slice = slice((i + 1) * hold_out_val, None)\n",
    "    # Validation sets\n",
    "    val_current_oc = data_oc[slice_val]\n",
    "    val_current_oa = data_oa[slice_val]\n",
    "    \n",
    "    # Training sets (excluding validation slices)\n",
    "    train_current_oc = data_oc[before_slice] + data_oc[after_slice]\n",
    "    train_current_oa = data_oa[before_slice] + data_oa[after_slice]\n",
    "    \n",
    "    # Combine oc and oa for the current fold\n",
    "    val_current = val_current_oc + val_current_oa\n",
    "    train_current = train_current_oc + train_current_oa\n",
    "    \n",
    "    val_targets = [0 for _ in val_current_oc] + [1 for _ in val_current_oa]\n",
    "    train_targets = [0 for _ in train_current_oc] + [1 for _ in train_current_oa]\n",
    "    if as_arrays:\n",
    "        return train_current_oc, train_current_oa, val_current_oc, val_current_oa\n",
    "    else:\n",
    "        train_set = SimpleDataset(train_current, train_targets, device)\n",
    "        val_set = SimpleDataset(val_current, val_targets, device)\n",
    "        \n",
    "        return train_set, val_set\n",
    "        "
   ],
   "id": "f43f4287afc3f932",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**1. Define the parameters of the CNN**"
   ],
   "id": "f9019154629579d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=1, lr=?):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = in_channels\n",
    "        self.lr = lr\n",
    "        self.define_network()\n",
    "    \n",
    "    def define_network(self):\n",
    "        # Define layers as a torch.nn.Sequential object\n",
    "        # This is pretty nice, since we can just go layers(input) to get output\n",
    "        # Rather than having a bunch of functions in the forward function\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.in_channels, out_channels=?, kernel_size=3, padding=1), # dim = in\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(out_features=?),    # Automatically infers the input dimension\n",
    "            nn.ReLU(),\n",
    "            ?, # Do whatever you like here, don't stick to the template!!\n",
    "            ?,\n",
    "            nn.Linear(in_features=?, out_features=?),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=?, out_features=self.num_classes),\n",
    "            \n",
    "        ).to(device)\n",
    "                \n",
    "        # Loss function and optimizer, as you know, Adam is the meta\n",
    "        self.criterion = ?  # What loss function do we use for classification?\n",
    "        self.optim = torch.optim.Adam(self.layers.parameters(), lr=self.lr)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    def train(self, train_dataloader, epochs=1, val_dataloader=None, run_filepath=\"\"):\n",
    "        \n",
    "        # To hold accuracy during training and testing\n",
    "        train_accs = []\n",
    "        test_accs = []\n",
    "        for epoch in tqdm(range(epochs), desc=f'Training'):\n",
    "            \n",
    "            path_epoch = join(run_filepath, f\"{epoch}.pth\")\n",
    "            os.makedirs(os.path.dirname(path_epoch), exist_ok=True)\n",
    "            if os.path.exists(path_epoch):\n",
    "                self.load_model(path_epoch)\n",
    "                print(f\"Loaded model from {path_epoch}. Skipping epoch.\")\n",
    "                \n",
    "            epoch_acc = 0\n",
    "\n",
    "            for inputs, targets in tqdm(train_dataloader):\n",
    "                logits = self(inputs)\n",
    "                loss = self.criterion(logits, targets)\n",
    "                loss.backward()\n",
    "\n",
    "                self.optim.step()\n",
    "                self.optim.zero_grad()\n",
    "\n",
    "                # Not actually used for training, just for keeping track of accuracy\n",
    "                epoch_acc += (torch.argmax(logits, dim=1) == targets).sum().item()\n",
    "            \n",
    "            self.save_model(path_epoch)\n",
    "            train_accs.append(epoch_acc / len(train_dataloader.dataset))\n",
    "            print(f\"Epoch {epoch} training accuracy: {epoch_acc / len(train_dataloader.dataset)}\")\n",
    "\n",
    "            # If we have val dataloader, we can evaluate after each epoch\n",
    "            if val_dataloader is not None:\n",
    "                acc = self.eval(val_dataloader)\n",
    "                test_accs.append(acc)\n",
    "                print(f\"Epoch {epoch} validation accuracy: {acc}\")\n",
    "        \n",
    "        return train_accs, test_accs\n",
    "\n",
    "    def eval(self, test_dataloader):\n",
    "        \n",
    "        total_acc = 0\n",
    "\n",
    "        for input_batch, label_batch in test_dataloader:\n",
    "            # Get predictions\n",
    "            logits = self(input_batch)\n",
    "\n",
    "            # Remember, outs are probabilities (so there's 10 for each input)\n",
    "            # The classification the network wants to assign, must therefore be the probability with the larget value\n",
    "            # We find that using argmax (dim=1, because dim=0 would be across batch dimension)\n",
    "            classifications = torch.argmax(logits, dim=1)\n",
    "            total_acc += (classifications == label_batch).sum().item()\n",
    "\n",
    "        total_acc = total_acc / len(test_dataloader.dataset)\n",
    "\n",
    "        return total_acc\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save({\n",
    "            'model_state_dict': self.state_dict(),\n",
    "            'optimizer_state_dict': self.optim.state_dict()\n",
    "        }, path)\n",
    "        print(f\"Model saved to {path}\")\n",
    "\n",
    "    # Load model method\n",
    "    def load_model(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optim.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        print(f\"Model loaded from {path}\")"
   ],
   "id": "dd3df8f0a6e59d65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**2. Define the parameters of a Feed Forward Neural Network**"
   ],
   "id": "9e8bba7a4f6fbbc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class FFNN(CNN):\n",
    "    def __init__(self, num_classes, in_channels=1, lr=?):\n",
    "        # This is lazy OOP and could be done better.\n",
    "        super().__init__(num_classes, in_channels, lr)\n",
    "        \n",
    "    def define_network(self):\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            ? # Up to you:))\n",
    "        ).to(device)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optim = torch.optim.Adam(self.layers.parameters(), lr=self.lr)"
   ],
   "id": "40f3dafa5ae3b2f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "*Boilerplate torch dataset definition*"
   ],
   "id": "abc38307a941714"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, time_data, targets, device, dtype_data=torch.float32, dtype_targets=torch.long):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            time_data (list or array): Training time data.\n",
    "            targets (list or array): Corresponding target values (labels).\n",
    "        \"\"\"\n",
    "        self.data = time_data\n",
    "        self.targets = targets\n",
    "        self.device = device\n",
    "        self.dtype_data = dtype_data\n",
    "        self.dtype_targets = dtype_targets\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples\n",
    "        assert len(self.data) == len(self.targets)\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve a single sample (time data and corresponding target)\n",
    "        sample = self.data[idx]\n",
    "        target = self.targets[idx]\n",
    "        sample = torch.tensor(sample).to(dtype=self.dtype_data).to(device)\n",
    "        target = torch.tensor(target).to(dtype=self.dtype_targets).to(device)\n",
    "        if sample.ndim == 2:\n",
    "            sample = sample.unsqueeze(0)\n",
    "        \n",
    "        # Optionally convert to torch tensors if needed\n",
    "        return sample, target"
   ],
   "id": "6a58353ede3f164",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**3. The following is a VERY simple way of calculating a threshold. What could you do to make it more sophisticated? (Feel free to implement you more sophisticated method!)**"
   ],
   "id": "f7231e768dfd0bdc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def simple_threshold_calculator(vec1, vec2):\n",
    "    m1 = np.mean(vec1)\n",
    "    m2 = np.mean(vec2)\n",
    "    # TODO: Implement a more sophisticated method\n",
    "    return np.mean([m1, m2])"
   ],
   "id": "f4f47649735968e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**4. Now use the models you have defined above in cross-validation loops that each use the EEG data in different ways. Here we have set up the following 4 loops, but feel free to experiment with other configurations:**\n",
    "\n",
    "- Threshold on alpha energy\n",
    "- CNN on frequency domain\n",
    "- CNN on Time domain\n",
    "- FFNN on frequency domain\n",
    "\n",
    "*The pytorch models will be saved in a folder named 'models' and can be loaded from there. The test loop automatically loads a model if it has the same name as the one it is about to train thus skipping the epoch.*"
   ],
   "id": "5a9afc040f8eacab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "hold_out_val = 1\n",
    "hold_out_test = 2\n",
    "\n",
    "# Assuming the oc and oa data are already concatenated, split them back to separate.\n",
    "train_set_time_oc = data_oc_optical_channels[:-hold_out_test]\n",
    "train_set_time_oa = data_oa_optical_channels[:-hold_out_test]\n",
    "\n",
    "train_set_frequency_oc = [el[1] for el in power_spectra_oc[:-hold_out_test]]\n",
    "train_set_frequency_oa = [el[1] for el in power_spectra_oa[:-hold_out_test]]\n",
    "\n",
    "train_set_alpha_oc = alphas_oc[:-hold_out_test]\n",
    "train_set_alpha_oa = alphas_oa[:-hold_out_test]\n",
    "\n",
    "k_folds = len(train_set_alpha_oc) // hold_out_val"
   ],
   "id": "42a941d77d0a3e98",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "run_name = join(os.getcwd(), \"models\", \"testing-freq-threshold\")\n",
    "\n",
    "val_accuracies_threshold = []\n",
    "for i in tqdm(range(k_folds), desc=f\"Cross-validation {os.path.basename(run_name)}\"):\n",
    "    # Combine oc and oa for the current fold\n",
    "    train_oc, train_oa, val_oc, val_oa = cross_validation_split(alphas_oc.tolist(), alphas_oa.tolist(), hold_out_val, i, device=device, as_arrays=True)\n",
    "    \n",
    "    threshold = simple_threshold_calculator(train_oc, train_oa)\n",
    "    acc = (val_oc > threshold) + (threshold < val_oa)\n",
    "    val_accuracies_threshold.append(acc)"
   ],
   "id": "ff10f11080f4fb86",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "run_name = join(os.getcwd(), \"models\", \"testing-time-CNN\")\n",
    "val_accuracies_cnn_time = []\n",
    "epochs_CNN_time = 1\n",
    "# Cross-validation loop\n",
    "for i in tqdm(range(k_folds), desc=f\"Cross-validation {os.path.basename(run_name)}\"):\n",
    "    # Combine oc and oa for the current fold\n",
    "    train_set_current, val_set_current = cross_validation_split(train_set_time_oc, train_set_time_oa, hold_out_val, i, device=device)\n",
    "    \n",
    "    CNN_time = CNN(num_classes=2)\n",
    "    CNN_time.train(DataLoader(train_set_current), epochs=epochs_CNN_time, run_filepath=f\"{run_name}-{i}\")\n",
    "    val_cnn_time = CNN_time.eval(DataLoader(val_set_current))\n",
    "    val_accuracies_cnn_time.append(val_cnn_time)"
   ],
   "id": "b857ba379b98b879",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "run_name = join(os.getcwd(), \"models\", \"testing-freq-CNN\")\n",
    "val_accuracies_cnn_freq = []\n",
    "epochs_CNN_freq = 10\n",
    "\n",
    "for i in tqdm(range(k_folds), desc=f\"Cross-validation {os.path.basename(run_name)}\"):\n",
    "    # Combine oc and oa for the current fold\n",
    "    train_set_current, val_set_current = cross_validation_split(train_set_frequency_oc, train_set_frequency_oa, hold_out_val, i, device=device)\n",
    "    \n",
    "    CNN_freq = CNN(num_classes=2)\n",
    "    CNN_freq.train(DataLoader(train_set_current), epochs=epochs_CNN_freq, run_filepath=f\"{run_name}-{i}\")\n",
    "    val_cnn_freq = CNN_freq.eval(DataLoader(val_set_current))\n",
    "    val_accuracies_cnn_freq.append(val_cnn_freq)"
   ],
   "id": "83bac708ea3d9e68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "run_name = join(os.getcwd(), \"models\", \"testing-freq-FFNN\")\n",
    "epochs_FFNN = 2\n",
    "val_accuracies_fnn_freq = []\n",
    "for i in tqdm(range(k_folds), desc=f\"Cross-validation {os.path.basename(run_name)}\"):\n",
    "    # Combine oc and oa for the current fold\n",
    "    train_set_current, val_set_current = cross_validation_split(train_set_frequency_oc, train_set_frequency_oa, hold_out_val, i, device=device)\n",
    "    \n",
    "    FFNN_freq = FFNN(num_classes=2)\n",
    "    FFNN_freq.train(DataLoader(train_set_current), epochs=epochs_FFNN, run_filepath=f\"{run_name}-{i}\")\n",
    "    val_cnn_freq = FFNN_freq.eval(DataLoader(val_set_current))\n",
    "    val_accuracies_fnn_freq.append(val_cnn_freq)"
   ],
   "id": "8b2e627fd7b215e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate means\n",
    "# Small value to prevent division by zero\n",
    "epsilon = 1e-8\n",
    "\n",
    "# Calculate means\n",
    "mean_cnn_time = [sum(val_accuracies_cnn_time) / (len(val_accuracies_cnn_time) + epsilon)] * len(val_accuracies_cnn_time)\n",
    "mean_cnn_freq = [sum(val_accuracies_cnn_freq) / (len(val_accuracies_cnn_freq) + epsilon)] * len(val_accuracies_cnn_freq)\n",
    "mean_fnn_freq = [sum(val_accuracies_fnn_freq) / (len(val_accuracies_fnn_freq) + epsilon)] * len(val_accuracies_fnn_freq)\n",
    "mean_threshold = [sum(val_accuracies_threshold) / (len(val_accuracies_threshold) + epsilon)] * len(val_accuracies_threshold)\n",
    "\n",
    "# Plotting\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "cnn_time_color = 'blue'  # Example color for CNN Time\n",
    "cnn_freq_color = 'orange'  # Example color for CNN Freq\n",
    "fnn_freq_color = 'green'  # Example color for FNN Freq\n",
    "threshold_color = 'red'  # Example color for Threshold\n",
    "\n",
    "plt.plot(val_accuracies_cnn_time, label='CNN Time', marker='o', color=cnn_time_color)\n",
    "plt.plot(val_accuracies_cnn_freq, label='CNN Freq', marker='s', color=cnn_freq_color)\n",
    "plt.plot(val_accuracies_fnn_freq, label='FNN Freq', marker='^', color=fnn_freq_color)\n",
    "plt.plot(val_accuracies_threshold, label='Threshold', marker='d', color=threshold_color)\n",
    "\n",
    "# Plot means with low alpha, matching colors\n",
    "plt.plot(mean_cnn_time, label='Mean CNN Time', linestyle='--', alpha=0.3, color=cnn_time_color)\n",
    "plt.plot(mean_cnn_freq, label='Mean CNN Freq', linestyle='--', alpha=0.3, color=cnn_freq_color)\n",
    "plt.plot(mean_fnn_freq, label='Mean FNN Freq', linestyle='--', alpha=0.3, color=fnn_freq_color)\n",
    "plt.plot(mean_threshold, label='Mean Threshold', linestyle='--', alpha=0.3, color=threshold_color)\n",
    "\n",
    "plt.title('Validation Accuracies with Means')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "19205c9ad57ecba9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**5. How did it go?**"
   ],
   "id": "f0459050619dac94"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
