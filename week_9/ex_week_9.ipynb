{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from scipy.linalg import eigh\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises week 11 - word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are very popular representations for the vocabulary of any document corpus. It is one of the major breakthroughs of utilizing deep learning to solve challenging problems in natural language processing. A word embedding is a learned representation for the text, in which the words that have similar meaning have a similar representation. This meaning can be based varius aspects such as syntax, semantics and abstract relations between words.\n",
    "\n",
    "For this exercise we use pre-trained word embedding from the GloVe (Global Vectors for Word Representation, https://nlp.stanford.edu/projects/glove/) project. In particular we use data trained on the socalled Wikipedia 2014 and Gigaword 5 corpus. This dataset contains $400000$ words, each represented by a feature vector (http://nlp.stanford.edu/data/glove.6B.zip). \n",
    "\n",
    "**Problems**\n",
    "- Exercise 1 generate co-occurrence matrix for the dataset \"Reviews.csv\".\n",
    "- Exercise 2 compares the word embeddings for different words and examine whether the word embedding captures various types of relations between the words.\n",
    "- Exercise 3 uses principal component analysis to reduce the dimensionality of the word embedding.\n",
    "- Exercise 4 uses transfer learning to address the problem of identifying analogies between words, utilizing the knowledge comprised in the word embedding.\n",
    "- Exercise 5 illustrates whether the word embedding allows us to infer general conclusions about the original text corpus, by utilizing the methodology from the first exercises. In particular we examine wheter biases or stereotypes have been convayed to the word embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise 1 (Optional) - Co-ocurance matrix\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data    = pd.read_csv(\"Reviews.csv\")\n",
    "reviews = data[\"Text\"].values[:4] # take only first couple of reviews"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Find the distinct words\n",
    "\n",
    "Hints:\n",
    "- loop over all words in each review\n",
    "- save only the distinct words in a list\n",
    "- remember to convert all words to lower characters using \".lower()\" on strings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for review in reviews:\n",
    "    ?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calculate $X_{ij}$\n",
    "Hints:\n",
    "- for each distinct word loop through all reviews again\n",
    "- for each review find the location where that distinct word is using \"np.where(words_in_review==distinct)[0]\"\n",
    "- take the previous $L=1$ and subsequent $L=1$ words of those locations and add 1 to the co-occurence matrix. You can use pandas DataFrame for indexing properly.\n",
    "- optional: make your implementation handle $L$ to be an arbitrary positive integer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_ij        = ?\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot $X$ as an image"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.imshow(X_ij.to_numpy())\n",
    "plt.colorbar()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Glove\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Loading the data set**\n",
    "\n",
    "The following code loads the data and stores it in a *dictionary*. A dictionary is an abstract datastructure that stores key-value pairs. Given a key, the corresponding value can be stored or retrieved. The datastructure hence defines a list of keys and values and implements the functionality to link keys to values. For the dictionary declared in the code, the keys are the individual words (as strings) and the values are the associated GloVe feature vectors (as one dimensional numpy arrays).\n",
    "\n",
    "$\\star$ In the code, the data origines from a text file, where each line describes one word and its associated feature vector. Make sure you understand the string manipulation that is used to parse the data, within the for-loop in the code.\n",
    "\n",
    "$\\star$ When a feature vector is defined, the function np.asarray is given the argument \"float32\". Why is this necessary?\n",
    "\n",
    "$\\star$ What is the dimensionality of the data? "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"glove.6B.50d.txt\"\n",
    "\n",
    "dictionary = {}\n",
    "with open(filename,'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        elements = line.split();\n",
    "        word = elements[0];\n",
    "        vector = np.asarray(elements[1:],\"float32\")\n",
    "        dictionary[word] = vector;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Word similarities\n",
    "\n",
    "\n",
    "\n",
    "In this exercise you will inspect the word embedding for different words, in order to determine whether words that semantically have similar meaning (or are semantically related) also have similar representations under the embedding.\n",
    "\n",
    "To compare two words, we use the cosine similarity score between the feature vectors representing the two words.\n",
    "The following code implements two functions. *word_similarity* computes the similarity between two words while *nearest_neighbours* computes the $N$ nearest neighbours (most similar words) to a single word.\n",
    "\n",
    "$\\star$ Carefully examine the implementation of the two functions. Make sure you understand the code line by line.\n",
    "\n",
    "The next cell calls the *word_similarity* function for different pairs of words withen various groups of words.\n",
    "\n",
    "$\\star$ Based on your common understanding, discuss the relation within the individual groups. One of the groups is called 'baseline'.\n",
    "\n",
    "$\\star$ Compare the similarity scores for the word pairs within each group. Do the relative differenses in scores reflect your own intuition? Does this inform you about certain relations captured by the word embedding? For instance, what can you learn from observing that 'Denmark' is more similar to 'Sweden' than to 'Copenhagen'?\n",
    "\n",
    "$\\star$ Instead of basing the similarity on the entire feature vector, try to use only the first five or ten values. Comment on whether you observe different outcomes.\n",
    "\n",
    "$\\star$ Select representative words from each group and use the *nearest_neighbours* function to find the respectively most similar words from the entire vocabulary. Try to examine words that you would expect to be both relatively similar and relatively different within the groups. Note why you intuitively would expect the words to be similar or different.\n",
    "Comment on your findings and compare you results both within and between groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(v1, v2):\n",
    "    return 1 - spatial.distance.cosine(v1,v2);\n",
    "\n",
    "def word_similarity(word1, word2):    \n",
    "    v1 = dictionary[word1]\n",
    "    v2 = dictionary[word2]\n",
    "    similarity =  cos_sim(v1,v2)\n",
    "    print(\"similarity score between\",word1,\"and\",word2,\"=\",similarity)\n",
    "    \n",
    "def nearest_neighbours(word, num_neighbours):    \n",
    "    words = np.asarray(list(dictionary.keys()))\n",
    "    scores = np.zeros(len(words))    \n",
    "    #find similarity score for all words in the dictionary \n",
    "    v = dictionary[word]\n",
    "    for id in range(len(words)):\n",
    "        word2 = words[id]\n",
    "        v2 = dictionary[word2]\n",
    "        scores[id] = cos_sim(v,v2)\n",
    "    #sort scores and words according to score\n",
    "    sorted_scores, sorted_words = zip(*sorted(zip(scores,words)))\n",
    "    sorted_scores = sorted_scores[::-1]\n",
    "    sorted_words = sorted_words[::-1]\n",
    "    #print most similar words from the vocabulary\n",
    "    print(\"most similar words to \",word)\n",
    "    for x in range(num_neighbours):\n",
    "        print(sorted_scores[x],\"\\t\",sorted_words[x])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#humans and family structures\n",
    "word_similarity(\"king\",\"queen\")\n",
    "word_similarity(\"prince\",\"princess\")\n",
    "word_similarity(\"prince\",\"brother\")\n",
    "word_similarity(\"princess\",\"sister\")\n",
    "word_similarity(\"prince\",\"sister\")\n",
    "word_similarity(\"princess\",\"brother\")\n",
    "\n",
    "word_similarity(\"brother\",\"sister\")\n",
    "word_similarity(\"brother\",\"son\")\n",
    "word_similarity(\"sister\",\"daughter\")\n",
    "word_similarity(\"brother\",\"daughter\")\n",
    "word_similarity(\"sister\",\"son\")\n",
    "\n",
    "#prepositions and orientations\n",
    "word_similarity(\"after\",\"before\")\n",
    "word_similarity(\"over\",\"under\")\n",
    "word_similarity(\"vertical\",\"horizontal\")\n",
    "word_similarity(\"left\",\"right\")\n",
    "\n",
    "#toponyms (places)\n",
    "word_similarity(\"city\",\"country\")\n",
    "word_similarity(\"city\",\"town\")\n",
    "word_similarity(\"denmark\",\"sweden\")\n",
    "word_similarity(\"denmark\",\"copenhagen\")\n",
    "word_similarity(\"stockholm\",\"copenhagen\")\n",
    "word_similarity(\"stockholm\",\"denmark\")\n",
    "word_similarity(\"sweden\",\"stockholm\")\n",
    "word_similarity(\"sweden\",\"copenhagen\")\n",
    "word_similarity(\"walk\",\"run\")\n",
    "word_similarity(\"run\",\"sit\")\n",
    "word_similarity(\"walk\",\"sit\")\n",
    "\n",
    "#Homonyms (words with different meanings)\n",
    "word_similarity(\"second\",\"minute\")\n",
    "word_similarity(\"first\",\"second\")\n",
    "word_similarity(\"first\",\"minute\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - Visualizing using PCA\n",
    "\n",
    "In this exercise we use principal component analysis (PCA) to reduce the dimensionality of the feature vector space. This allows us to illustrate the spatial relation between different words from the vocabulary.\n",
    "\n",
    "A function, *plot_pca* that conducts PCA on a sub-set of the data and projects these data-points onto the first two principal components is given below. The next cell then calls the *plot_pca* function for different lists of words. You are encuraged to modify the content of the lists if you expect that it may further support your findings in the following questions.\n",
    "\n",
    "$\\star$ Examine the code and observe how PCA is conducted.\n",
    "\n",
    "$\\star$ Comment on any linearity and distance between pairs of words.\n",
    "\n",
    "$\\star$ When visualizing for many words, comment on which words are grouped together. Does the grouping capture any semantic meaning?\n",
    "\n",
    "$\\star$ Evaluate whether you can use the PCA visualization to illustrate your findings from the previous exercise. Can your previous intuitions be supported by the plots?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca(word_list):\n",
    "    \n",
    "    #obtain dictionary of words from word_list\n",
    "    sub_dictionary = {key:dictionary[key] for key in word_list}\n",
    "    words = sub_dictionary.keys()\n",
    "    vectors = np.asarray([sub_dictionary[key] for key in words]).T\n",
    "    \n",
    "    #pca on feature vectors for selected words\n",
    "    mean_vector = np.mean(vectors,axis=1)\n",
    "    data = vectors - mean_vector[:,None]\n",
    "    #compute the covariance matrix\n",
    "    S = np.cov(data)\n",
    "    #obtain eigenvectors and eigenvalues\n",
    "    eigenValues, eigenVectors = eigh(S)\n",
    "    #sort according to size of eigenvalues\n",
    "    eigenValues = eigenValues[::-1]\n",
    "    eigenVectors = eigenVectors[:, ::-1]\n",
    "    Y = eigenVectors.T@(data)\n",
    "        \n",
    "    #plot for first two principal components\n",
    "    for label, x, y in zip(word_list, Y[0,:], Y[1,:]):\n",
    "        plt.plot(x,y,\"*\")\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca([\"ship\",\"harbor\",\"car\",\"garage\"])\n",
    "plot_pca([\"father\",\"mother\",\"son\",\"daughter\"])\n",
    "plot_pca([\"king\",\"queen\",\"prince\",\"princess\",\"man\",\"woman\",\"boy\",\"girl\"])\n",
    "plot_pca([\"denmark\",\"copenhagen\",\"sweden\",\"stockholm\",\"germany\",\"berlin\",\"france\",\"paris\"])\n",
    "plot_pca([\"guitar\",\"piano\",\"saxophone\",\"cat\",\"dog\",\"horse\",\"tiger\",\"book\",\"article\",\"journal\",\"computer\",\"telephone\",\"dishwasher\",\"gramaphone\",\"house\",\"garden\",\"lawn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 - Analogy solver\n",
    "\n",
    "\n",
    "As defined by Aristotle as \"an equality of proportions\", an analogy can be considered a problem involving four terms, such that the second term is related to the first in a similar way as the forth term is related to the third.\n",
    "\n",
    "An example of an analogy is;\n",
    "- *evening* is to *morning* as *dinner* is to *breakfast*\n",
    "\n",
    "Analogies are often used as verbal reasoning tests, where the test subject is asked to infer the similarity between two relations, by only being presented the first three terms of the analogy problem.\n",
    "For instance, the subject is expected to infer the term *girl* from the problem;\n",
    "- *brother* is to *sister* as *boy* is to *???*\n",
    "\n",
    "These kind of problems are often included in assessments of intelligence (such as IQ or children development tests) as the ability to draw analogies are believed to reflect the underlying cognitive ability to represent and reason about higher-order relations.\n",
    "\n",
    "In this exercise you are going to examine an AI algorithm for solving such analogy problems. As input, the algorithm is given the first three terms of an analogy problem. It will then rely on the linear substructure of the GloVe word embedding to find and return the best candidate word for the forth term.\n",
    "\n",
    "**Algorithm:**\n",
    "For a given problem, let $v_1,v_2,v_3$ be the vector representations for the first three terms. \n",
    "The guess for the forth term is found as the word for which the associated vector representation is most similar to the vector $v_d = (v_3 - (v_1-v_2))$. To compare vectors, the algorithm uses the cosine similarity measure.\n",
    "\n",
    "$\\star$ Argue why this approach is reasonable and why $v_d$ is computed as it is. Hint: It may be easier to build intuition from considering the problem in 2D.\n",
    "\n",
    "$\\star$ Test the algorithm with the provided examples as well as with analogy problems of your own. Investigate whether there are particular types of analogies that the algorithm cannot solve. Comment on why this is.\n",
    "\n",
    "$\\star$ Try to modify the algorithm so it returns the five 'best guesses'. Comment on whether the correct answer is among these guesses, in particular for the problems it could not solve before. \n",
    "\n",
    "$\\star$ For real cognitive tests the analogy problem is often presented as a multiple-choise question. Modifying the algorithm to such a setting, would you expect it to perform better? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#algorithm for solving analogy problems\n",
    "def find_analogy(w1, w2, w3):\n",
    "    v1 = dictionary[w1];\n",
    "    v2 = dictionary[w2];\n",
    "    v3 = dictionary[w3];\n",
    "    vd = v3 - (v1-v2);    \n",
    "    max_similarity = -10000\n",
    "    for word in dictionary.keys():\n",
    "        #avoid guesses on already used terms\n",
    "        if word!=w1 and word!=w2 and word!=w3:\n",
    "            v = dictionary[word]\n",
    "            similarity = 1 - spatial.distance.cosine(vd,v)\n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity;\n",
    "                bestguess_word = word;\n",
    "    return bestguess_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#examples of analogy problems\n",
    "\n",
    "#w1 = \"man\"; w2 = \"woman\"; w3 = \"king\";\n",
    "#w1 = \"brother\"; w2 = \"sister\"; w3 = \"boy\";\n",
    "#w1 = \"pen\"; w2 = \"paper\"; w3 = \"doctor\";\n",
    "#w1 = \"denmark\"; w2 = \"copenhagen\"; w3 = \"germany\";\n",
    "#w1 = \"denmark\"; w2 = \"copenhagen\"; w3 = \"france\";\n",
    "#w1 = \"denmark\"; w2 = \"france\"; w3 = \"copenhagen\";\n",
    "#w1 = \"car\"; w2 = \"parking\"; w3 = \"ship\";\n",
    "#w1 = \"left\"; w2 = \"right\"; w3 = \"horizontal\";\n",
    "#w1 = \"woman\"; w2 = \"man\"; w3 = \"mother\";\n",
    "#w1 = \"man\"; w2 = \"woman\"; w3 = \"doctor\";\n",
    "\n",
    "w4 = find_analogy(w1,w2,w3)\n",
    "print(w1,\"is to\",w2,\"as\",w3,\"is to\",w4)\n",
    "plot_pca([w1,w2,w3,w4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 - Quantifying stereotypes\n",
    "\n",
    "As for most machine learning algorithms, word embedding can be proned to biases. If the original data contained systematic biases, the learned representations may also reflect these biases. The GloVe machine learning algorithm was trained on Wikipedia texts. If these texts in general contained or described specific stereotypes, the word embedding will also capture these stereotypes. \n",
    "Often it is found that word embeddings contain gender stereotypes, often significantly with regards to professions.\n",
    "\n",
    "$\\star$ Use the plot_pca function to visualize the words \"nurse\", \"doctor\", \"man\" and \"woman\". Comment on the plot in terms of gender stereotypes. \n",
    "\n",
    "$\\star$ Try to compare the cosine distance between various professions (\"programmer\", \"secretary\", \"pilot\", \"president\", \"nurse\",...) and the words \"mother\" and \"farther\". Comment on your findings.\n",
    "\n",
    "$\\star$ Evaluate on the terms that the word embedding relate to the two words \"man\" and \"woman\". For instance you can look at and compare the nearest neighbours to \"man\" and \"woman\" respectively. Are there any professions related to \"man\" and \"woman\"? Do \"man\" and \"woman\" have neighbours in common? \n",
    "\n",
    "\n",
    "$\\star$ Discuss whether you can use the analogy solver to test for various biases or stereotypes that may have been convayed to the word embedding.\n",
    "\n",
    "$\\star$ Reflect on whether word embedding in general is a feasible approach to verify if a text corpus contains biases or stereotypes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}