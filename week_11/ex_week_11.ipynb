{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention\n",
    "\n",
    "*In this exercise we are going to work with attention. If you haven't heard, it has become **massively** important, mostly since transformers use it (and is part of what makes them the new black for everything).*\n",
    "\n",
    "After these exercises (and the lecture), you should be able to (in rough order of importance):\n",
    "- Understand what attention is\n",
    "- Understand how attention is computed given inputs \n",
    "- Understand why it is often called 'self-attention'\n",
    "- Understand how neural networks are used to get attention values\n",
    "- Understand why attention is good to use\n",
    "- Understand how attention differs from just embeddings\n",
    "\n",
    "- Understand early (non learnable) vs newer learnable attention\n",
    "\n",
    "## Quick introduction\n",
    "\n",
    "# Fix indexing here\n",
    "\n",
    "*In its most basic form, attention is a form of similarity measur applied to sequences of usually words. As you may know, the dot product is a rough measure of similarity between vectors. Let $\\mathbf{y}$ be an input sequence of word embeddings, for example a sentence in english, and let $\\mathbf{x}$ be the same sentence but in another language. The similarity between two words in either sequences will then be:*\n",
    "\n",
    "$$e_{ij} = x^{(i)^T} y^{(j)}$$\n",
    "\n",
    "*That is, how much word $\\mathbf{x}^{(i)}$ relates to word $\\mathbf{y}^{(j)}$ The softmax is then usually used to scale these values to be between 0 and 1, which gives the attention weight $W_{ij}$ between word $i$ and $j$*:\n",
    "\n",
    "$$W_{ij} = \\text{softmax}\\left(\\mathbf{x}^{(i)^T} \\mathbf{y}^{(j)}\\right)$$\n",
    "\n",
    "*After this, the attention weights $W_{ij}$ can then be used on each word of the input sentence, to get how much they relate to those in the desired output sentence:*\n",
    "\n",
    "$$\\mathbf{o}{(i)} = \\sum^T_{j = 0} W_{ij} \\mathbf{x}^{j}$$\n",
    "\n",
    "*The result $o^{(i)}$, will then be **a vector** representing how much each word in the input sentence relates to **a word** in the output sentence. If we wanted to get an attention value **from each word** to **each word** we would need a matrix, which would be the whole $o$*\n",
    "\n",
    "## Self-attention\n",
    "\n",
    "*Introduced primarily in [attention is all you need](https://arxiv.org/abs/1706.03762), self attention improves 'regular' attention by adding learnable parameters to the attention.*\n",
    "\n",
    "*Note that above, the attention values only depended on the similarity between words, meaning words embedded with similar values also got high attention scores. This doesn't make sense for many tasks, particularly in translation, since some words in sentences can relate a lot to others despite being different, consider the sentence:*\n",
    "\n",
    "***Mary saw a dog in the window, she wanted it***\n",
    "\n",
    "*Logically, we can think the words (Mary, saw, wanted, it), (dog, it), (in, window) fit together as they describe the relationship that Mary wanted **it** \"it\" meaning \"the dog\" (not the window). This would not be caught by the above method, since it is uncommon that \"it\" and \"dog\" are very similar words...*\n",
    "\n",
    "*The solution is to use **self-attention**. Here we maintain three new embedding values for each word: $\\mathbf{Q}$ (queries), $\\mathbf{K}$ (keys), and $\\mathbf{V}$ (values). Do not worrry about the names.* \n",
    "\n",
    "*We train these three embeddings much the same way as we would word embeddings in, say Fasttext. However, they are not specifically embeddings. **They work on embeddings**. To get $\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}$ for one input sentence (of words!!) $\\mathbf{s}$, we first get a word embedding of that input sentence, and then multiply that matrix with matrices for each $\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}$:*\n",
    "\n",
    "$$\\mathbf{Q} = \\text{embedding}(\\mathbf{s})W_Q,\\quad \\mathbf{K} = \\text{embedding}(\\mathbf{s})W_K, \\quad \\mathbf{V} = \\text{embedding}(\\mathbf{s})W_V$$\n",
    "\n",
    "*We then calculate the full attention matrix:*\n",
    "\n",
    "$$\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left( \\frac{\\mathbf{Q} \\mathbf{K}^T}{\\sqrt{d_k}} \\right) \\mathbf{V}$$\n",
    "\n",
    "*The reason we scale by $\\sqrt{d_k}$ (which is either based on the dimension of $W_K$, chosen arbitrarily, or based on the number of 'heads' for multi-head attention), is simply to avoid exploding gradients (don't worry too much about it)...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Calculating attention\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
